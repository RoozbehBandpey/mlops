{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U keras-tuner\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================]ETA:  - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - ETA:  - ETA:  - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values between 0 and 1\n",
    "img_train = img_train.astype('float32') / 255.0\n",
    "img_test = img_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "  \n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "  model.add(keras.layers.Dense(units = hp_units, activation = 'relu'))\n",
    "  model.add(keras.layers.Dense(10))\n",
    "\n",
    "  # Tune the learning rate for the optimizer \n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "  \n",
    "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "                metrics = ['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = 'val_accuracy', \n",
    "                     max_epochs = 10,\n",
    "                     factor = 3,\n",
    "                     directory = 'my_dir',\n",
    "                     project_name = 'intro_to_kt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "  def on_train_end(*args, **kwargs):\n",
    "    IPython.display.clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 0a9c598f4961d9d2af1909664138f108</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.8770999908447266</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.0001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/bracket: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/epochs: 10</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/initial_epoch: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/round: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units: 384</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 448 and the optimal learning rate for the optimizer\n",
      "is 0.001.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner.search(img_train, label_train, epochs = 10, validation_data = (img_test, label_test), callbacks = [ClearTrainingOutput()])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - ETA: 6:39 - loss: 2.4182 - accuracy: 0.18 - ETA: 16s - loss: 1.1096 - accuracy: 0.6250 - ETA: 9s - loss: 0.9439 - accuracy: 0.680 - ETA: 7s - loss: 0.8509 - accuracy: 0.71 - ETA: 6s - loss: 0.7915 - accuracy: 0.73 - ETA: 5s - loss: 0.7635 - accuracy: 0.73 - ETA: 4s - loss: 0.7490 - accuracy: 0.74 - ETA: 4s - loss: 0.7154 - accuracy: 0.75 - ETA: 4s - loss: 0.6931 - accuracy: 0.75 - ETA: 4s - loss: 0.6791 - accuracy: 0.76 - ETA: 3s - loss: 0.6614 - accuracy: 0.76 - ETA: 3s - loss: 0.6459 - accuracy: 0.77 - ETA: 3s - loss: 0.6365 - accuracy: 0.77 - ETA: 3s - loss: 0.6296 - accuracy: 0.77 - ETA: 3s - loss: 0.6191 - accuracy: 0.78 - ETA: 3s - loss: 0.6134 - accuracy: 0.78 - ETA: 3s - loss: 0.6100 - accuracy: 0.78 - ETA: 2s - loss: 0.6092 - accuracy: 0.78 - ETA: 2s - loss: 0.6000 - accuracy: 0.78 - ETA: 2s - loss: 0.5933 - accuracy: 0.79 - ETA: 2s - loss: 0.5874 - accuracy: 0.79 - ETA: 2s - loss: 0.5814 - accuracy: 0.79 - ETA: 2s - loss: 0.5796 - accuracy: 0.79 - ETA: 2s - loss: 0.5777 - accuracy: 0.79 - ETA: 2s - loss: 0.5721 - accuracy: 0.80 - ETA: 2s - loss: 0.5674 - accuracy: 0.80 - ETA: 2s - loss: 0.5649 - accuracy: 0.80 - ETA: 2s - loss: 0.5611 - accuracy: 0.80 - ETA: 2s - loss: 0.5575 - accuracy: 0.80 - ETA: 2s - loss: 0.5523 - accuracy: 0.80 - ETA: 2s - loss: 0.5509 - accuracy: 0.80 - ETA: 1s - loss: 0.5460 - accuracy: 0.80 - ETA: 1s - loss: 0.5428 - accuracy: 0.80 - ETA: 1s - loss: 0.5403 - accuracy: 0.80 - ETA: 1s - loss: 0.5380 - accuracy: 0.81 - ETA: 1s - loss: 0.5349 - accuracy: 0.81 - ETA: 1s - loss: 0.5322 - accuracy: 0.81 - ETA: 1s - loss: 0.5300 - accuracy: 0.81 - ETA: 1s - loss: 0.5271 - accuracy: 0.81 - ETA: 1s - loss: 0.5253 - accuracy: 0.81 - ETA: 1s - loss: 0.5223 - accuracy: 0.81 - ETA: 1s - loss: 0.5197 - accuracy: 0.81 - ETA: 1s - loss: 0.5171 - accuracy: 0.81 - ETA: 1s - loss: 0.5146 - accuracy: 0.81 - ETA: 1s - loss: 0.5132 - accuracy: 0.81 - ETA: 1s - loss: 0.5107 - accuracy: 0.81 - ETA: 1s - loss: 0.5076 - accuracy: 0.81 - ETA: 0s - loss: 0.5061 - accuracy: 0.82 - ETA: 0s - loss: 0.5037 - accuracy: 0.82 - ETA: 0s - loss: 0.5022 - accuracy: 0.82 - ETA: 0s - loss: 0.5008 - accuracy: 0.82 - ETA: 0s - loss: 0.4980 - accuracy: 0.82 - ETA: 0s - loss: 0.4961 - accuracy: 0.82 - ETA: 0s - loss: 0.4945 - accuracy: 0.82 - ETA: 0s - loss: 0.4930 - accuracy: 0.82 - ETA: 0s - loss: 0.4908 - accuracy: 0.82 - ETA: 0s - loss: 0.4882 - accuracy: 0.82 - ETA: 0s - loss: 0.4869 - accuracy: 0.82 - ETA: 0s - loss: 0.4859 - accuracy: 0.82 - ETA: 0s - loss: 0.4844 - accuracy: 0.82 - ETA: 0s - loss: 0.4830 - accuracy: 0.82 - ETA: 0s - loss: 0.4806 - accuracy: 0.82 - ETA: 0s - loss: 0.4786 - accuracy: 0.82 - ETA: 0s - loss: 0.4771 - accuracy: 0.82 - ETA: 0s - loss: 0.4762 - accuracy: 0.82 - 4s 66us/sample - loss: 0.4745 - accuracy: 0.8303 - val_loss: 0.4045 - val_accuracy: 0.8511\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - ETA: 5s - loss: 0.4900 - accuracy: 0.78 - ETA: 3s - loss: 0.3891 - accuracy: 0.85 - ETA: 3s - loss: 0.4021 - accuracy: 0.85 - ETA: 3s - loss: 0.3799 - accuracy: 0.86 - ETA: 3s - loss: 0.3743 - accuracy: 0.86 - ETA: 3s - loss: 0.3779 - accuracy: 0.86 - ETA: 3s - loss: 0.3740 - accuracy: 0.86 - ETA: 2s - loss: 0.3728 - accuracy: 0.86 - ETA: 2s - loss: 0.3735 - accuracy: 0.86 - ETA: 2s - loss: 0.3717 - accuracy: 0.86 - ETA: 2s - loss: 0.3714 - accuracy: 0.86 - ETA: 2s - loss: 0.3739 - accuracy: 0.86 - ETA: 2s - loss: 0.3693 - accuracy: 0.86 - ETA: 2s - loss: 0.3665 - accuracy: 0.86 - ETA: 2s - loss: 0.3700 - accuracy: 0.86 - ETA: 2s - loss: 0.3736 - accuracy: 0.86 - ETA: 2s - loss: 0.3718 - accuracy: 0.86 - ETA: 2s - loss: 0.3720 - accuracy: 0.86 - ETA: 2s - loss: 0.3741 - accuracy: 0.86 - ETA: 2s - loss: 0.3738 - accuracy: 0.86 - ETA: 2s - loss: 0.3741 - accuracy: 0.86 - ETA: 2s - loss: 0.3729 - accuracy: 0.86 - ETA: 2s - loss: 0.3731 - accuracy: 0.86 - ETA: 2s - loss: 0.3728 - accuracy: 0.86 - ETA: 2s - loss: 0.3740 - accuracy: 0.86 - ETA: 2s - loss: 0.3728 - accuracy: 0.86 - ETA: 1s - loss: 0.3718 - accuracy: 0.86 - ETA: 1s - loss: 0.3713 - accuracy: 0.86 - ETA: 1s - loss: 0.3711 - accuracy: 0.86 - ETA: 1s - loss: 0.3697 - accuracy: 0.86 - ETA: 1s - loss: 0.3673 - accuracy: 0.86 - ETA: 1s - loss: 0.3668 - accuracy: 0.86 - ETA: 1s - loss: 0.3667 - accuracy: 0.86 - ETA: 1s - loss: 0.3654 - accuracy: 0.86 - ETA: 1s - loss: 0.3633 - accuracy: 0.86 - ETA: 1s - loss: 0.3614 - accuracy: 0.86 - ETA: 1s - loss: 0.3604 - accuracy: 0.86 - ETA: 1s - loss: 0.3603 - accuracy: 0.86 - ETA: 1s - loss: 0.3626 - accuracy: 0.86 - ETA: 1s - loss: 0.3628 - accuracy: 0.86 - ETA: 1s - loss: 0.3624 - accuracy: 0.86 - ETA: 1s - loss: 0.3625 - accuracy: 0.86 - ETA: 1s - loss: 0.3622 - accuracy: 0.86 - ETA: 1s - loss: 0.3623 - accuracy: 0.86 - ETA: 1s - loss: 0.3622 - accuracy: 0.86 - ETA: 0s - loss: 0.3625 - accuracy: 0.86 - ETA: 0s - loss: 0.3618 - accuracy: 0.86 - ETA: 0s - loss: 0.3616 - accuracy: 0.86 - ETA: 0s - loss: 0.3609 - accuracy: 0.86 - ETA: 0s - loss: 0.3610 - accuracy: 0.86 - ETA: 0s - loss: 0.3604 - accuracy: 0.86 - ETA: 0s - loss: 0.3598 - accuracy: 0.86 - ETA: 0s - loss: 0.3591 - accuracy: 0.86 - ETA: 0s - loss: 0.3582 - accuracy: 0.86 - ETA: 0s - loss: 0.3590 - accuracy: 0.86 - ETA: 0s - loss: 0.3586 - accuracy: 0.86 - ETA: 0s - loss: 0.3591 - accuracy: 0.86 - ETA: 0s - loss: 0.3602 - accuracy: 0.86 - ETA: 0s - loss: 0.3599 - accuracy: 0.86 - ETA: 0s - loss: 0.3599 - accuracy: 0.86 - ETA: 0s - loss: 0.3589 - accuracy: 0.86 - ETA: 0s - loss: 0.3587 - accuracy: 0.86 - ETA: 0s - loss: 0.3582 - accuracy: 0.86 - ETA: 0s - loss: 0.3585 - accuracy: 0.86 - ETA: 0s - loss: 0.3587 - accuracy: 0.86 - 4s 61us/sample - loss: 0.3589 - accuracy: 0.8687 - val_loss: 0.4482 - val_accuracy: 0.8363\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - ETA: 4s - loss: 0.6915 - accuracy: 0.71 - ETA: 3s - loss: 0.3205 - accuracy: 0.89 - ETA: 3s - loss: 0.3379 - accuracy: 0.87 - ETA: 3s - loss: 0.3443 - accuracy: 0.87 - ETA: 2s - loss: 0.3299 - accuracy: 0.87 - ETA: 2s - loss: 0.3297 - accuracy: 0.87 - ETA: 2s - loss: 0.3260 - accuracy: 0.88 - ETA: 2s - loss: 0.3250 - accuracy: 0.87 - ETA: 2s - loss: 0.3243 - accuracy: 0.88 - ETA: 2s - loss: 0.3243 - accuracy: 0.88 - ETA: 2s - loss: 0.3250 - accuracy: 0.87 - ETA: 2s - loss: 0.3217 - accuracy: 0.88 - ETA: 2s - loss: 0.3203 - accuracy: 0.88 - ETA: 2s - loss: 0.3209 - accuracy: 0.88 - ETA: 2s - loss: 0.3228 - accuracy: 0.88 - ETA: 2s - loss: 0.3237 - accuracy: 0.88 - ETA: 2s - loss: 0.3230 - accuracy: 0.88 - ETA: 2s - loss: 0.3213 - accuracy: 0.88 - ETA: 2s - loss: 0.3204 - accuracy: 0.88 - ETA: 2s - loss: 0.3213 - accuracy: 0.88 - ETA: 2s - loss: 0.3229 - accuracy: 0.88 - ETA: 2s - loss: 0.3232 - accuracy: 0.88 - ETA: 2s - loss: 0.3236 - accuracy: 0.88 - ETA: 2s - loss: 0.3230 - accuracy: 0.88 - ETA: 2s - loss: 0.3224 - accuracy: 0.88 - ETA: 2s - loss: 0.3222 - accuracy: 0.88 - ETA: 2s - loss: 0.3234 - accuracy: 0.88 - ETA: 1s - loss: 0.3213 - accuracy: 0.88 - ETA: 1s - loss: 0.3228 - accuracy: 0.88 - ETA: 1s - loss: 0.3216 - accuracy: 0.88 - ETA: 1s - loss: 0.3224 - accuracy: 0.88 - ETA: 1s - loss: 0.3227 - accuracy: 0.88 - ETA: 1s - loss: 0.3226 - accuracy: 0.88 - ETA: 1s - loss: 0.3227 - accuracy: 0.88 - ETA: 1s - loss: 0.3239 - accuracy: 0.88 - ETA: 1s - loss: 0.3245 - accuracy: 0.88 - ETA: 1s - loss: 0.3244 - accuracy: 0.88 - ETA: 1s - loss: 0.3238 - accuracy: 0.88 - ETA: 1s - loss: 0.3240 - accuracy: 0.88 - ETA: 1s - loss: 0.3237 - accuracy: 0.88 - ETA: 1s - loss: 0.3244 - accuracy: 0.88 - ETA: 1s - loss: 0.3244 - accuracy: 0.88 - ETA: 1s - loss: 0.3240 - accuracy: 0.88 - ETA: 1s - loss: 0.3239 - accuracy: 0.88 - ETA: 1s - loss: 0.3237 - accuracy: 0.88 - ETA: 1s - loss: 0.3228 - accuracy: 0.88 - ETA: 0s - loss: 0.3218 - accuracy: 0.88 - ETA: 0s - loss: 0.3221 - accuracy: 0.88 - ETA: 0s - loss: 0.3220 - accuracy: 0.88 - ETA: 0s - loss: 0.3220 - accuracy: 0.88 - ETA: 0s - loss: 0.3220 - accuracy: 0.88 - ETA: 0s - loss: 0.3224 - accuracy: 0.88 - ETA: 0s - loss: 0.3227 - accuracy: 0.88 - ETA: 0s - loss: 0.3232 - accuracy: 0.88 - ETA: 0s - loss: 0.3232 - accuracy: 0.88 - ETA: 0s - loss: 0.3237 - accuracy: 0.88 - ETA: 0s - loss: 0.3236 - accuracy: 0.88 - ETA: 0s - loss: 0.3233 - accuracy: 0.88 - ETA: 0s - loss: 0.3229 - accuracy: 0.88 - ETA: 0s - loss: 0.3229 - accuracy: 0.88 - ETA: 0s - loss: 0.3233 - accuracy: 0.88 - ETA: 0s - loss: 0.3235 - accuracy: 0.88 - ETA: 0s - loss: 0.3237 - accuracy: 0.88 - ETA: 0s - loss: 0.3238 - accuracy: 0.88 - ETA: 0s - loss: 0.3235 - accuracy: 0.88 - ETA: 0s - loss: 0.3244 - accuracy: 0.88 - 4s 62us/sample - loss: 0.3243 - accuracy: 0.8811 - val_loss: 0.3717 - val_accuracy: 0.8639\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - ETA: 5s - loss: 0.1412 - accuracy: 0.96 - ETA: 3s - loss: 0.3110 - accuracy: 0.88 - ETA: 3s - loss: 0.3067 - accuracy: 0.88 - ETA: 3s - loss: 0.3186 - accuracy: 0.88 - ETA: 3s - loss: 0.3157 - accuracy: 0.88 - ETA: 3s - loss: 0.3188 - accuracy: 0.88 - ETA: 3s - loss: 0.3156 - accuracy: 0.88 - ETA: 2s - loss: 0.3121 - accuracy: 0.88 - ETA: 2s - loss: 0.3093 - accuracy: 0.88 - ETA: 2s - loss: 0.3089 - accuracy: 0.88 - ETA: 2s - loss: 0.3080 - accuracy: 0.88 - ETA: 2s - loss: 0.3071 - accuracy: 0.88 - ETA: 2s - loss: 0.3071 - accuracy: 0.88 - ETA: 2s - loss: 0.3071 - accuracy: 0.88 - ETA: 2s - loss: 0.3062 - accuracy: 0.88 - ETA: 2s - loss: 0.3068 - accuracy: 0.88 - ETA: 2s - loss: 0.3058 - accuracy: 0.88 - ETA: 2s - loss: 0.3057 - accuracy: 0.88 - ETA: 2s - loss: 0.3059 - accuracy: 0.88 - ETA: 2s - loss: 0.3048 - accuracy: 0.88 - ETA: 2s - loss: 0.3062 - accuracy: 0.88 - ETA: 2s - loss: 0.3042 - accuracy: 0.88 - ETA: 2s - loss: 0.3053 - accuracy: 0.88 - ETA: 2s - loss: 0.3048 - accuracy: 0.88 - ETA: 2s - loss: 0.3029 - accuracy: 0.88 - ETA: 2s - loss: 0.3022 - accuracy: 0.88 - ETA: 2s - loss: 0.3029 - accuracy: 0.88 - ETA: 2s - loss: 0.3013 - accuracy: 0.88 - ETA: 1s - loss: 0.3021 - accuracy: 0.88 - ETA: 1s - loss: 0.3027 - accuracy: 0.88 - ETA: 1s - loss: 0.3024 - accuracy: 0.88 - ETA: 1s - loss: 0.3035 - accuracy: 0.88 - ETA: 1s - loss: 0.3037 - accuracy: 0.88 - ETA: 1s - loss: 0.3044 - accuracy: 0.88 - ETA: 1s - loss: 0.3039 - accuracy: 0.88 - ETA: 1s - loss: 0.3043 - accuracy: 0.88 - ETA: 1s - loss: 0.3046 - accuracy: 0.88 - ETA: 1s - loss: 0.3030 - accuracy: 0.88 - ETA: 1s - loss: 0.3038 - accuracy: 0.88 - ETA: 1s - loss: 0.3028 - accuracy: 0.88 - ETA: 1s - loss: 0.3023 - accuracy: 0.88 - ETA: 1s - loss: 0.3023 - accuracy: 0.88 - ETA: 1s - loss: 0.3024 - accuracy: 0.88 - ETA: 1s - loss: 0.3021 - accuracy: 0.88 - ETA: 1s - loss: 0.3019 - accuracy: 0.88 - ETA: 1s - loss: 0.3019 - accuracy: 0.88 - ETA: 1s - loss: 0.3012 - accuracy: 0.88 - ETA: 1s - loss: 0.3010 - accuracy: 0.88 - ETA: 0s - loss: 0.3012 - accuracy: 0.88 - ETA: 0s - loss: 0.3016 - accuracy: 0.88 - ETA: 0s - loss: 0.3010 - accuracy: 0.88 - ETA: 0s - loss: 0.3018 - accuracy: 0.88 - ETA: 0s - loss: 0.3019 - accuracy: 0.88 - ETA: 0s - loss: 0.3018 - accuracy: 0.88 - ETA: 0s - loss: 0.3012 - accuracy: 0.88 - ETA: 0s - loss: 0.3020 - accuracy: 0.88 - ETA: 0s - loss: 0.3026 - accuracy: 0.88 - ETA: 0s - loss: 0.3028 - accuracy: 0.88 - ETA: 0s - loss: 0.3024 - accuracy: 0.88 - ETA: 0s - loss: 0.3021 - accuracy: 0.88 - ETA: 0s - loss: 0.3016 - accuracy: 0.88 - ETA: 0s - loss: 0.3017 - accuracy: 0.88 - ETA: 0s - loss: 0.3017 - accuracy: 0.88 - ETA: 0s - loss: 0.3013 - accuracy: 0.88 - ETA: 0s - loss: 0.3008 - accuracy: 0.88 - ETA: 0s - loss: 0.3005 - accuracy: 0.88 - ETA: 0s - loss: 0.2999 - accuracy: 0.88 - 4s 63us/sample - loss: 0.3001 - accuracy: 0.8886 - val_loss: 0.3407 - val_accuracy: 0.8756\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - ETA: 5s - loss: 0.3969 - accuracy: 0.87 - ETA: 3s - loss: 0.2884 - accuracy: 0.89 - ETA: 3s - loss: 0.2724 - accuracy: 0.90 - ETA: 3s - loss: 0.2700 - accuracy: 0.90 - ETA: 3s - loss: 0.2668 - accuracy: 0.90 - ETA: 3s - loss: 0.2666 - accuracy: 0.90 - ETA: 3s - loss: 0.2678 - accuracy: 0.90 - ETA: 3s - loss: 0.2699 - accuracy: 0.89 - ETA: 2s - loss: 0.2742 - accuracy: 0.89 - ETA: 2s - loss: 0.2746 - accuracy: 0.89 - ETA: 2s - loss: 0.2790 - accuracy: 0.89 - ETA: 2s - loss: 0.2761 - accuracy: 0.89 - ETA: 2s - loss: 0.2760 - accuracy: 0.89 - ETA: 2s - loss: 0.2795 - accuracy: 0.89 - ETA: 2s - loss: 0.2792 - accuracy: 0.89 - ETA: 2s - loss: 0.2793 - accuracy: 0.89 - ETA: 2s - loss: 0.2775 - accuracy: 0.89 - ETA: 2s - loss: 0.2797 - accuracy: 0.89 - ETA: 2s - loss: 0.2826 - accuracy: 0.89 - ETA: 2s - loss: 0.2816 - accuracy: 0.89 - ETA: 2s - loss: 0.2816 - accuracy: 0.89 - ETA: 2s - loss: 0.2823 - accuracy: 0.89 - ETA: 2s - loss: 0.2826 - accuracy: 0.89 - ETA: 2s - loss: 0.2837 - accuracy: 0.89 - ETA: 2s - loss: 0.2840 - accuracy: 0.89 - ETA: 2s - loss: 0.2840 - accuracy: 0.89 - ETA: 2s - loss: 0.2826 - accuracy: 0.89 - ETA: 1s - loss: 0.2825 - accuracy: 0.89 - ETA: 1s - loss: 0.2844 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2847 - accuracy: 0.89 - ETA: 1s - loss: 0.2855 - accuracy: 0.89 - ETA: 1s - loss: 0.2854 - accuracy: 0.89 - ETA: 1s - loss: 0.2848 - accuracy: 0.89 - ETA: 1s - loss: 0.2845 - accuracy: 0.89 - ETA: 1s - loss: 0.2839 - accuracy: 0.89 - ETA: 1s - loss: 0.2843 - accuracy: 0.89 - ETA: 1s - loss: 0.2844 - accuracy: 0.89 - ETA: 1s - loss: 0.2836 - accuracy: 0.89 - ETA: 1s - loss: 0.2835 - accuracy: 0.89 - ETA: 1s - loss: 0.2839 - accuracy: 0.89 - ETA: 1s - loss: 0.2845 - accuracy: 0.89 - ETA: 1s - loss: 0.2844 - accuracy: 0.89 - ETA: 1s - loss: 0.2837 - accuracy: 0.89 - ETA: 1s - loss: 0.2838 - accuracy: 0.89 - ETA: 1s - loss: 0.2836 - accuracy: 0.89 - ETA: 0s - loss: 0.2830 - accuracy: 0.89 - ETA: 0s - loss: 0.2830 - accuracy: 0.89 - ETA: 0s - loss: 0.2827 - accuracy: 0.89 - ETA: 0s - loss: 0.2827 - accuracy: 0.89 - ETA: 0s - loss: 0.2819 - accuracy: 0.89 - ETA: 0s - loss: 0.2812 - accuracy: 0.89 - ETA: 0s - loss: 0.2809 - accuracy: 0.89 - ETA: 0s - loss: 0.2814 - accuracy: 0.89 - ETA: 0s - loss: 0.2817 - accuracy: 0.89 - ETA: 0s - loss: 0.2814 - accuracy: 0.89 - ETA: 0s - loss: 0.2814 - accuracy: 0.89 - ETA: 0s - loss: 0.2822 - accuracy: 0.89 - ETA: 0s - loss: 0.2819 - accuracy: 0.89 - ETA: 0s - loss: 0.2813 - accuracy: 0.89 - ETA: 0s - loss: 0.2811 - accuracy: 0.89 - ETA: 0s - loss: 0.2818 - accuracy: 0.89 - ETA: 0s - loss: 0.2815 - accuracy: 0.89 - ETA: 0s - loss: 0.2807 - accuracy: 0.89 - ETA: 0s - loss: 0.2812 - accuracy: 0.89 - ETA: 0s - loss: 0.2804 - accuracy: 0.89 - 4s 61us/sample - loss: 0.2807 - accuracy: 0.8958 - val_loss: 0.3446 - val_accuracy: 0.8763\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - ETA: 4s - loss: 0.2595 - accuracy: 0.93 - ETA: 3s - loss: 0.2761 - accuracy: 0.89 - ETA: 3s - loss: 0.2688 - accuracy: 0.89 - ETA: 3s - loss: 0.2668 - accuracy: 0.89 - ETA: 3s - loss: 0.2599 - accuracy: 0.90 - ETA: 3s - loss: 0.2632 - accuracy: 0.89 - ETA: 2s - loss: 0.2585 - accuracy: 0.89 - ETA: 2s - loss: 0.2585 - accuracy: 0.90 - ETA: 2s - loss: 0.2632 - accuracy: 0.89 - ETA: 2s - loss: 0.2615 - accuracy: 0.89 - ETA: 2s - loss: 0.2626 - accuracy: 0.89 - ETA: 2s - loss: 0.2634 - accuracy: 0.89 - ETA: 2s - loss: 0.2656 - accuracy: 0.89 - ETA: 2s - loss: 0.2618 - accuracy: 0.89 - ETA: 2s - loss: 0.2628 - accuracy: 0.89 - ETA: 2s - loss: 0.2647 - accuracy: 0.89 - ETA: 2s - loss: 0.2641 - accuracy: 0.89 - ETA: 2s - loss: 0.2654 - accuracy: 0.89 - ETA: 2s - loss: 0.2647 - accuracy: 0.89 - ETA: 2s - loss: 0.2644 - accuracy: 0.89 - ETA: 2s - loss: 0.2630 - accuracy: 0.89 - ETA: 2s - loss: 0.2641 - accuracy: 0.89 - ETA: 2s - loss: 0.2642 - accuracy: 0.89 - ETA: 2s - loss: 0.2639 - accuracy: 0.89 - ETA: 2s - loss: 0.2635 - accuracy: 0.89 - ETA: 1s - loss: 0.2639 - accuracy: 0.89 - ETA: 1s - loss: 0.2646 - accuracy: 0.89 - ETA: 1s - loss: 0.2643 - accuracy: 0.89 - ETA: 1s - loss: 0.2639 - accuracy: 0.89 - ETA: 1s - loss: 0.2632 - accuracy: 0.89 - ETA: 1s - loss: 0.2631 - accuracy: 0.89 - ETA: 1s - loss: 0.2634 - accuracy: 0.89 - ETA: 1s - loss: 0.2639 - accuracy: 0.89 - ETA: 1s - loss: 0.2643 - accuracy: 0.89 - ETA: 1s - loss: 0.2659 - accuracy: 0.89 - ETA: 1s - loss: 0.2662 - accuracy: 0.89 - ETA: 1s - loss: 0.2653 - accuracy: 0.89 - ETA: 1s - loss: 0.2656 - accuracy: 0.89 - ETA: 1s - loss: 0.2663 - accuracy: 0.89 - ETA: 1s - loss: 0.2663 - accuracy: 0.89 - ETA: 1s - loss: 0.2660 - accuracy: 0.89 - ETA: 1s - loss: 0.2648 - accuracy: 0.89 - ETA: 1s - loss: 0.2667 - accuracy: 0.89 - ETA: 1s - loss: 0.2669 - accuracy: 0.89 - ETA: 1s - loss: 0.2670 - accuracy: 0.89 - ETA: 0s - loss: 0.2668 - accuracy: 0.89 - ETA: 0s - loss: 0.2672 - accuracy: 0.89 - ETA: 0s - loss: 0.2669 - accuracy: 0.89 - ETA: 0s - loss: 0.2669 - accuracy: 0.89 - ETA: 0s - loss: 0.2670 - accuracy: 0.89 - ETA: 0s - loss: 0.2672 - accuracy: 0.89 - ETA: 0s - loss: 0.2670 - accuracy: 0.89 - ETA: 0s - loss: 0.2669 - accuracy: 0.89 - ETA: 0s - loss: 0.2672 - accuracy: 0.89 - ETA: 0s - loss: 0.2673 - accuracy: 0.89 - ETA: 0s - loss: 0.2680 - accuracy: 0.89 - ETA: 0s - loss: 0.2679 - accuracy: 0.89 - ETA: 0s - loss: 0.2678 - accuracy: 0.89 - ETA: 0s - loss: 0.2677 - accuracy: 0.89 - ETA: 0s - loss: 0.2682 - accuracy: 0.89 - ETA: 0s - loss: 0.2681 - accuracy: 0.89 - ETA: 0s - loss: 0.2679 - accuracy: 0.89 - ETA: 0s - loss: 0.2678 - accuracy: 0.89 - ETA: 0s - loss: 0.2681 - accuracy: 0.89 - ETA: 0s - loss: 0.2683 - accuracy: 0.89 - 4s 61us/sample - loss: 0.2683 - accuracy: 0.8983 - val_loss: 0.3200 - val_accuracy: 0.8852\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - ETA: 4s - loss: 0.3350 - accuracy: 0.87 - ETA: 3s - loss: 0.2136 - accuracy: 0.91 - ETA: 3s - loss: 0.2318 - accuracy: 0.91 - ETA: 3s - loss: 0.2417 - accuracy: 0.90 - ETA: 3s - loss: 0.2354 - accuracy: 0.91 - ETA: 3s - loss: 0.2366 - accuracy: 0.90 - ETA: 3s - loss: 0.2408 - accuracy: 0.90 - ETA: 3s - loss: 0.2463 - accuracy: 0.90 - ETA: 3s - loss: 0.2448 - accuracy: 0.90 - ETA: 3s - loss: 0.2400 - accuracy: 0.90 - ETA: 3s - loss: 0.2399 - accuracy: 0.90 - ETA: 3s - loss: 0.2384 - accuracy: 0.91 - ETA: 2s - loss: 0.2391 - accuracy: 0.90 - ETA: 2s - loss: 0.2397 - accuracy: 0.91 - ETA: 2s - loss: 0.2428 - accuracy: 0.90 - ETA: 2s - loss: 0.2473 - accuracy: 0.90 - ETA: 2s - loss: 0.2469 - accuracy: 0.90 - ETA: 2s - loss: 0.2485 - accuracy: 0.90 - ETA: 2s - loss: 0.2486 - accuracy: 0.90 - ETA: 2s - loss: 0.2488 - accuracy: 0.90 - ETA: 2s - loss: 0.2484 - accuracy: 0.90 - ETA: 2s - loss: 0.2495 - accuracy: 0.90 - ETA: 2s - loss: 0.2501 - accuracy: 0.90 - ETA: 2s - loss: 0.2509 - accuracy: 0.90 - ETA: 2s - loss: 0.2519 - accuracy: 0.90 - ETA: 2s - loss: 0.2517 - accuracy: 0.90 - ETA: 2s - loss: 0.2521 - accuracy: 0.90 - ETA: 2s - loss: 0.2506 - accuracy: 0.90 - ETA: 2s - loss: 0.2500 - accuracy: 0.90 - ETA: 1s - loss: 0.2494 - accuracy: 0.90 - ETA: 1s - loss: 0.2507 - accuracy: 0.90 - ETA: 1s - loss: 0.2510 - accuracy: 0.90 - ETA: 1s - loss: 0.2492 - accuracy: 0.90 - ETA: 1s - loss: 0.2503 - accuracy: 0.90 - ETA: 1s - loss: 0.2524 - accuracy: 0.90 - ETA: 1s - loss: 0.2522 - accuracy: 0.90 - ETA: 1s - loss: 0.2535 - accuracy: 0.90 - ETA: 1s - loss: 0.2541 - accuracy: 0.90 - ETA: 1s - loss: 0.2539 - accuracy: 0.90 - ETA: 1s - loss: 0.2534 - accuracy: 0.90 - ETA: 1s - loss: 0.2534 - accuracy: 0.90 - ETA: 1s - loss: 0.2524 - accuracy: 0.90 - ETA: 1s - loss: 0.2519 - accuracy: 0.90 - ETA: 1s - loss: 0.2514 - accuracy: 0.90 - ETA: 1s - loss: 0.2519 - accuracy: 0.90 - ETA: 1s - loss: 0.2533 - accuracy: 0.90 - ETA: 1s - loss: 0.2526 - accuracy: 0.90 - ETA: 1s - loss: 0.2524 - accuracy: 0.90 - ETA: 0s - loss: 0.2528 - accuracy: 0.90 - ETA: 0s - loss: 0.2536 - accuracy: 0.90 - ETA: 0s - loss: 0.2532 - accuracy: 0.90 - ETA: 0s - loss: 0.2526 - accuracy: 0.90 - ETA: 0s - loss: 0.2532 - accuracy: 0.90 - ETA: 0s - loss: 0.2535 - accuracy: 0.90 - ETA: 0s - loss: 0.2533 - accuracy: 0.90 - ETA: 0s - loss: 0.2535 - accuracy: 0.90 - ETA: 0s - loss: 0.2536 - accuracy: 0.90 - ETA: 0s - loss: 0.2539 - accuracy: 0.90 - ETA: 0s - loss: 0.2543 - accuracy: 0.90 - ETA: 0s - loss: 0.2540 - accuracy: 0.90 - ETA: 0s - loss: 0.2543 - accuracy: 0.90 - ETA: 0s - loss: 0.2545 - accuracy: 0.90 - ETA: 0s - loss: 0.2542 - accuracy: 0.90 - ETA: 0s - loss: 0.2537 - accuracy: 0.90 - ETA: 0s - loss: 0.2539 - accuracy: 0.90 - ETA: 0s - loss: 0.2545 - accuracy: 0.90 - ETA: 0s - loss: 0.2544 - accuracy: 0.90 - 4s 62us/sample - loss: 0.2541 - accuracy: 0.9051 - val_loss: 0.3406 - val_accuracy: 0.8807\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - ETA: 5s - loss: 0.0893 - accuracy: 0.96 - ETA: 3s - loss: 0.2249 - accuracy: 0.91 - ETA: 3s - loss: 0.2425 - accuracy: 0.91 - ETA: 3s - loss: 0.2341 - accuracy: 0.91 - ETA: 3s - loss: 0.2355 - accuracy: 0.91 - ETA: 3s - loss: 0.2364 - accuracy: 0.91 - ETA: 3s - loss: 0.2341 - accuracy: 0.91 - ETA: 3s - loss: 0.2291 - accuracy: 0.91 - ETA: 2s - loss: 0.2328 - accuracy: 0.91 - ETA: 2s - loss: 0.2364 - accuracy: 0.91 - ETA: 2s - loss: 0.2359 - accuracy: 0.91 - ETA: 2s - loss: 0.2366 - accuracy: 0.91 - ETA: 2s - loss: 0.2365 - accuracy: 0.91 - ETA: 2s - loss: 0.2390 - accuracy: 0.91 - ETA: 2s - loss: 0.2377 - accuracy: 0.91 - ETA: 2s - loss: 0.2404 - accuracy: 0.91 - ETA: 2s - loss: 0.2392 - accuracy: 0.91 - ETA: 2s - loss: 0.2376 - accuracy: 0.91 - ETA: 2s - loss: 0.2374 - accuracy: 0.91 - ETA: 2s - loss: 0.2387 - accuracy: 0.91 - ETA: 2s - loss: 0.2402 - accuracy: 0.91 - ETA: 2s - loss: 0.2400 - accuracy: 0.91 - ETA: 2s - loss: 0.2413 - accuracy: 0.91 - ETA: 2s - loss: 0.2405 - accuracy: 0.91 - ETA: 2s - loss: 0.2413 - accuracy: 0.91 - ETA: 2s - loss: 0.2402 - accuracy: 0.91 - ETA: 1s - loss: 0.2404 - accuracy: 0.91 - ETA: 1s - loss: 0.2409 - accuracy: 0.90 - ETA: 1s - loss: 0.2406 - accuracy: 0.90 - ETA: 1s - loss: 0.2394 - accuracy: 0.91 - ETA: 1s - loss: 0.2390 - accuracy: 0.91 - ETA: 1s - loss: 0.2392 - accuracy: 0.91 - ETA: 1s - loss: 0.2384 - accuracy: 0.91 - ETA: 1s - loss: 0.2397 - accuracy: 0.91 - ETA: 1s - loss: 0.2396 - accuracy: 0.91 - ETA: 1s - loss: 0.2399 - accuracy: 0.91 - ETA: 1s - loss: 0.2391 - accuracy: 0.91 - ETA: 1s - loss: 0.2402 - accuracy: 0.90 - ETA: 1s - loss: 0.2403 - accuracy: 0.90 - ETA: 1s - loss: 0.2398 - accuracy: 0.90 - ETA: 1s - loss: 0.2403 - accuracy: 0.90 - ETA: 1s - loss: 0.2405 - accuracy: 0.90 - ETA: 1s - loss: 0.2404 - accuracy: 0.90 - ETA: 1s - loss: 0.2403 - accuracy: 0.90 - ETA: 1s - loss: 0.2399 - accuracy: 0.90 - ETA: 1s - loss: 0.2398 - accuracy: 0.91 - ETA: 0s - loss: 0.2399 - accuracy: 0.91 - ETA: 0s - loss: 0.2393 - accuracy: 0.91 - ETA: 0s - loss: 0.2400 - accuracy: 0.91 - ETA: 0s - loss: 0.2404 - accuracy: 0.91 - ETA: 0s - loss: 0.2407 - accuracy: 0.90 - ETA: 0s - loss: 0.2411 - accuracy: 0.90 - ETA: 0s - loss: 0.2406 - accuracy: 0.90 - ETA: 0s - loss: 0.2403 - accuracy: 0.90 - ETA: 0s - loss: 0.2404 - accuracy: 0.90 - ETA: 0s - loss: 0.2399 - accuracy: 0.90 - ETA: 0s - loss: 0.2399 - accuracy: 0.90 - ETA: 0s - loss: 0.2404 - accuracy: 0.90 - ETA: 0s - loss: 0.2403 - accuracy: 0.90 - ETA: 0s - loss: 0.2412 - accuracy: 0.90 - ETA: 0s - loss: 0.2417 - accuracy: 0.90 - ETA: 0s - loss: 0.2412 - accuracy: 0.90 - ETA: 0s - loss: 0.2418 - accuracy: 0.91 - ETA: 0s - loss: 0.2420 - accuracy: 0.91 - ETA: 0s - loss: 0.2420 - accuracy: 0.91 - ETA: 0s - loss: 0.2421 - accuracy: 0.91 - ETA: 0s - loss: 0.2427 - accuracy: 0.90 - 4s 63us/sample - loss: 0.2426 - accuracy: 0.9096 - val_loss: 0.3221 - val_accuracy: 0.8857\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - ETA: 5s - loss: 0.0699 - accuracy: 0.96 - ETA: 3s - loss: 0.2126 - accuracy: 0.91 - ETA: 3s - loss: 0.2040 - accuracy: 0.92 - ETA: 3s - loss: 0.2140 - accuracy: 0.92 - ETA: 3s - loss: 0.2182 - accuracy: 0.91 - ETA: 3s - loss: 0.2197 - accuracy: 0.91 - ETA: 2s - loss: 0.2160 - accuracy: 0.91 - ETA: 2s - loss: 0.2175 - accuracy: 0.91 - ETA: 2s - loss: 0.2199 - accuracy: 0.91 - ETA: 2s - loss: 0.2203 - accuracy: 0.91 - ETA: 2s - loss: 0.2169 - accuracy: 0.91 - ETA: 2s - loss: 0.2176 - accuracy: 0.91 - ETA: 2s - loss: 0.2198 - accuracy: 0.91 - ETA: 2s - loss: 0.2185 - accuracy: 0.91 - ETA: 2s - loss: 0.2178 - accuracy: 0.91 - ETA: 2s - loss: 0.2175 - accuracy: 0.91 - ETA: 2s - loss: 0.2179 - accuracy: 0.91 - ETA: 2s - loss: 0.2183 - accuracy: 0.91 - ETA: 2s - loss: 0.2193 - accuracy: 0.91 - ETA: 2s - loss: 0.2192 - accuracy: 0.91 - ETA: 2s - loss: 0.2201 - accuracy: 0.91 - ETA: 2s - loss: 0.2222 - accuracy: 0.91 - ETA: 2s - loss: 0.2222 - accuracy: 0.91 - ETA: 2s - loss: 0.2231 - accuracy: 0.91 - ETA: 2s - loss: 0.2228 - accuracy: 0.91 - ETA: 2s - loss: 0.2219 - accuracy: 0.91 - ETA: 1s - loss: 0.2226 - accuracy: 0.91 - ETA: 1s - loss: 0.2246 - accuracy: 0.91 - ETA: 1s - loss: 0.2236 - accuracy: 0.91 - ETA: 1s - loss: 0.2246 - accuracy: 0.91 - ETA: 1s - loss: 0.2236 - accuracy: 0.91 - ETA: 1s - loss: 0.2234 - accuracy: 0.91 - ETA: 1s - loss: 0.2241 - accuracy: 0.91 - ETA: 1s - loss: 0.2239 - accuracy: 0.91 - ETA: 1s - loss: 0.2248 - accuracy: 0.91 - ETA: 1s - loss: 0.2254 - accuracy: 0.91 - ETA: 1s - loss: 0.2260 - accuracy: 0.91 - ETA: 1s - loss: 0.2264 - accuracy: 0.91 - ETA: 1s - loss: 0.2268 - accuracy: 0.91 - ETA: 1s - loss: 0.2277 - accuracy: 0.91 - ETA: 1s - loss: 0.2281 - accuracy: 0.91 - ETA: 1s - loss: 0.2292 - accuracy: 0.91 - ETA: 1s - loss: 0.2297 - accuracy: 0.91 - ETA: 1s - loss: 0.2297 - accuracy: 0.91 - ETA: 1s - loss: 0.2296 - accuracy: 0.91 - ETA: 1s - loss: 0.2304 - accuracy: 0.91 - ETA: 1s - loss: 0.2312 - accuracy: 0.91 - ETA: 0s - loss: 0.2317 - accuracy: 0.91 - ETA: 0s - loss: 0.2322 - accuracy: 0.91 - ETA: 0s - loss: 0.2322 - accuracy: 0.91 - ETA: 0s - loss: 0.2325 - accuracy: 0.91 - ETA: 0s - loss: 0.2332 - accuracy: 0.91 - ETA: 0s - loss: 0.2331 - accuracy: 0.91 - ETA: 0s - loss: 0.2333 - accuracy: 0.91 - ETA: 0s - loss: 0.2331 - accuracy: 0.91 - ETA: 0s - loss: 0.2326 - accuracy: 0.91 - ETA: 0s - loss: 0.2321 - accuracy: 0.91 - ETA: 0s - loss: 0.2321 - accuracy: 0.91 - ETA: 0s - loss: 0.2322 - accuracy: 0.91 - ETA: 0s - loss: 0.2328 - accuracy: 0.91 - ETA: 0s - loss: 0.2329 - accuracy: 0.91 - ETA: 0s - loss: 0.2330 - accuracy: 0.91 - ETA: 0s - loss: 0.2331 - accuracy: 0.91 - ETA: 0s - loss: 0.2330 - accuracy: 0.91 - ETA: 0s - loss: 0.2326 - accuracy: 0.91 - ETA: 0s - loss: 0.2325 - accuracy: 0.91 - ETA: 0s - loss: 0.2324 - accuracy: 0.91 - 4s 63us/sample - loss: 0.2325 - accuracy: 0.9130 - val_loss: 0.3269 - val_accuracy: 0.8863\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - ETA: 6s - loss: 0.1969 - accuracy: 0.93 - ETA: 3s - loss: 0.2350 - accuracy: 0.91 - ETA: 3s - loss: 0.2290 - accuracy: 0.91 - ETA: 3s - loss: 0.2230 - accuracy: 0.91 - ETA: 3s - loss: 0.2248 - accuracy: 0.91 - ETA: 3s - loss: 0.2198 - accuracy: 0.91 - ETA: 2s - loss: 0.2198 - accuracy: 0.91 - ETA: 2s - loss: 0.2189 - accuracy: 0.91 - ETA: 2s - loss: 0.2212 - accuracy: 0.91 - ETA: 2s - loss: 0.2183 - accuracy: 0.91 - ETA: 2s - loss: 0.2167 - accuracy: 0.91 - ETA: 2s - loss: 0.2189 - accuracy: 0.91 - ETA: 2s - loss: 0.2161 - accuracy: 0.91 - ETA: 2s - loss: 0.2150 - accuracy: 0.91 - ETA: 2s - loss: 0.2161 - accuracy: 0.91 - ETA: 2s - loss: 0.2148 - accuracy: 0.91 - ETA: 2s - loss: 0.2144 - accuracy: 0.91 - ETA: 2s - loss: 0.2155 - accuracy: 0.91 - ETA: 2s - loss: 0.2169 - accuracy: 0.91 - ETA: 2s - loss: 0.2188 - accuracy: 0.91 - ETA: 2s - loss: 0.2198 - accuracy: 0.91 - ETA: 2s - loss: 0.2196 - accuracy: 0.91 - ETA: 2s - loss: 0.2189 - accuracy: 0.91 - ETA: 2s - loss: 0.2198 - accuracy: 0.91 - ETA: 2s - loss: 0.2201 - accuracy: 0.91 - ETA: 2s - loss: 0.2204 - accuracy: 0.91 - ETA: 2s - loss: 0.2197 - accuracy: 0.91 - ETA: 2s - loss: 0.2183 - accuracy: 0.91 - ETA: 1s - loss: 0.2185 - accuracy: 0.91 - ETA: 1s - loss: 0.2195 - accuracy: 0.91 - ETA: 1s - loss: 0.2209 - accuracy: 0.91 - ETA: 1s - loss: 0.2210 - accuracy: 0.91 - ETA: 1s - loss: 0.2208 - accuracy: 0.91 - ETA: 1s - loss: 0.2216 - accuracy: 0.91 - ETA: 1s - loss: 0.2217 - accuracy: 0.91 - ETA: 1s - loss: 0.2224 - accuracy: 0.91 - ETA: 1s - loss: 0.2226 - accuracy: 0.91 - ETA: 1s - loss: 0.2235 - accuracy: 0.91 - ETA: 1s - loss: 0.2244 - accuracy: 0.91 - ETA: 1s - loss: 0.2252 - accuracy: 0.91 - ETA: 1s - loss: 0.2258 - accuracy: 0.91 - ETA: 1s - loss: 0.2250 - accuracy: 0.91 - ETA: 1s - loss: 0.2253 - accuracy: 0.91 - ETA: 1s - loss: 0.2243 - accuracy: 0.91 - ETA: 1s - loss: 0.2236 - accuracy: 0.91 - ETA: 1s - loss: 0.2235 - accuracy: 0.91 - ETA: 1s - loss: 0.2238 - accuracy: 0.91 - ETA: 0s - loss: 0.2238 - accuracy: 0.91 - ETA: 0s - loss: 0.2235 - accuracy: 0.91 - ETA: 0s - loss: 0.2234 - accuracy: 0.91 - ETA: 0s - loss: 0.2241 - accuracy: 0.91 - ETA: 0s - loss: 0.2240 - accuracy: 0.91 - ETA: 0s - loss: 0.2233 - accuracy: 0.91 - ETA: 0s - loss: 0.2230 - accuracy: 0.91 - ETA: 0s - loss: 0.2231 - accuracy: 0.91 - ETA: 0s - loss: 0.2229 - accuracy: 0.91 - ETA: 0s - loss: 0.2228 - accuracy: 0.91 - ETA: 0s - loss: 0.2229 - accuracy: 0.91 - ETA: 0s - loss: 0.2229 - accuracy: 0.91 - ETA: 0s - loss: 0.2222 - accuracy: 0.91 - ETA: 0s - loss: 0.2224 - accuracy: 0.91 - ETA: 0s - loss: 0.2227 - accuracy: 0.91 - ETA: 0s - loss: 0.2226 - accuracy: 0.91 - ETA: 0s - loss: 0.2229 - accuracy: 0.91 - ETA: 0s - loss: 0.2232 - accuracy: 0.91 - ETA: 0s - loss: 0.2230 - accuracy: 0.91 - ETA: 0s - loss: 0.2229 - accuracy: 0.91 - 4s 62us/sample - loss: 0.2229 - accuracy: 0.9153 - val_loss: 0.3217 - val_accuracy: 0.8863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f461d7dd320>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(img_train, label_train, epochs = 10, validation_data = (img_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
