{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import shutil\n",
    "import azureml\n",
    "\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace, Run\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.dnn import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-20.1.1-py2.py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.0.2\n",
      "    Uninstalling pip-20.0.2:\n",
      "      Successfully uninstalled pip-20.0.2\n",
      "Successfully installed pip-20.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2.0\n",
      "  Downloading tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |█████████████████████           | 338.3 MB 91.9 MB/s eta 0:00:024     |███████▊                        | 125.0 MB 110.9 MB/s eta 0:00:04     |███████████████                 | 242.4 MB 96.2 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 516.2 MB 7.0 kB/s s eta 0:00:01��█████████████████████  | 483.7 MB 108.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 42.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.29.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (3.12.1)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 45.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (3.2.1)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (2.2.1)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (0.9.0)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow==2.2.0) (46.4.0.post20200518)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.6.0.post3)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.16.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
      "\u001b[31mERROR: tensorflow-gpu 2.1.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-gpu 2.1.0 has requirement tensorboard<2.2.0,>=2.1.0, but you'll have tensorboard 2.2.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-gpu 2.1.0 has requirement tensorflow-estimator<2.2.0,>=2.1.0rc0, but you'll have tensorflow-estimator 2.2.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: h5py, tensorflow-estimator, gast, astunparse, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.9.0\n",
      "    Uninstalling h5py-2.9.0:\n",
      "      Successfully uninstalled h5py-2.9.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.2.2\n",
      "    Uninstalling gast-0.2.2:\n",
      "      Successfully uninstalled gast-0.2.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.1.0\n",
      "    Uninstalling tensorflow-2.1.0:\n",
      "      Successfully uninstalled tensorflow-2.1.0\n",
      "Successfully installed astunparse-1.6.3 gast-0.3.3 h5py-2.10.0 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement azureml.logging (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for azureml.logging\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install azureml.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='gputraining', subscription_id='c46a9435-c957-4e6c-a0f4-b9a597984773', resource_group='mlops')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = './tf-mnist'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "exp = Experiment(workspace=ws, name='tf-mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "web_paths = [\n",
    "            'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "            'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "            'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "            'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "            ]\n",
    "dataset = Dataset.File.from_files(path=web_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/http/yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
       " '/http/yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
       " '/http/yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
       " '/http/yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.register(workspace=ws,\n",
    "                           name='mnist dataset',\n",
    "                           description='training and test dataset',\n",
    "                           create_new_version=True)\n",
    "\n",
    "# list the files referenced by dataset\n",
    "dataset.to_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n"
     ]
    }
   ],
   "source": [
    "cluster_name = \"gpucluster1\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - framework_version is not specified, defaulting to version 1.13.\n",
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['azureml-dataprep']. We cannot guarantee image build will succeed.\n"
     ]
    }
   ],
   "source": [
    "script_params = {\n",
    "    '--data-folder': dataset.as_named_input('mnist').as_mount(),\n",
    "    '--batch-size': 50,\n",
    "    '--first-layer-neurons': 300,\n",
    "    '--second-layer-neurons': 100,\n",
    "    '--learning-rate': 0.01\n",
    "}\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 entry_script='tf_mnist.py',\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 use_gpu=True,\n",
    "                 pip_packages=['azureml-dataprep[pandas,fuse]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: tf-mnist_1594515227_43c16e73\n",
      "Web View: https://ml.azure.com/experiments/tf-mnist/runs/tf-mnist_1594515227_43c16e73?wsid=/subscriptions/c46a9435-c957-4e6c-a0f4-b9a597984773/resourcegroups/mlops/workspaces/gputraining\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt\n",
      "========================================================================================================================\n",
      "\n",
      "2020-07-12T00:54:08Z Executing 'Copy ACR Details file' on 10.0.0.4\n",
      "2020-07-12T00:54:08Z Starting output-watcher...\n",
      "2020-07-12T00:54:09Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2020-07-12T00:54:09Z Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_1b21012d99b68bea171596e10bcaefa4\n",
      "f7277927d38a: Already exists\n",
      "8d3eac894db4: Already exists\n",
      "edf72af6d627: Already exists\n",
      "3e4f86211d23: Already exists\n",
      "d6e9603ff777: Already exists\n",
      "5cad422780e2: Already exists\n",
      "8130687c8acb: Already exists\n",
      "c11e9246d621: Already exists\n",
      "0dfae24cbbd9: Already exists\n",
      "0bb049a6d391: Already exists\n",
      "22a53069998a: Pulling fs layer\n",
      "db550b9db251: Pulling fs layer\n",
      "10a826755d7e: Pulling fs layer\n",
      "457c96476eac: Pulling fs layer\n",
      "e2d7418d62ad: Pulling fs layer\n",
      "a0d6fa61e0b7: Pulling fs layer\n",
      "93f75153ba2d: Pulling fs layer\n",
      "57257bbf7ef3: Pulling fs layer\n",
      "69627a2639c6: Pulling fs layer\n",
      "6c5f0a7c49e2: Pulling fs layer\n",
      "5b3e203b7296: Pulling fs layer\n",
      "a35b01115e7d: Pulling fs layer\n",
      "fd3bdfa06834: Pulling fs layer\n",
      "457c96476eac: Waiting\n",
      "69627a2639c6: Waiting\n",
      "93f75153ba2d: Waiting\n",
      "e2d7418d62ad: Waiting\n",
      "a0d6fa61e0b7: Waiting\n",
      "6c5f0a7c49e2: Waiting\n",
      "57257bbf7ef3: Waiting\n",
      "5b3e203b7296: Waiting\n",
      "a35b01115e7d: Waiting\n",
      "fd3bdfa06834: Waiting\n",
      "db550b9db251: Verifying Checksum\n",
      "db550b9db251: Download complete\n",
      "22a53069998a: Download complete\n",
      "10a826755d7e: Verifying Checksum\n",
      "10a826755d7e: Download complete\n",
      "a0d6fa61e0b7: Verifying Checksum\n",
      "a0d6fa61e0b7: Download complete\n",
      "457c96476eac: Verifying Checksum\n",
      "457c96476eac: Download complete\n",
      "93f75153ba2d: Verifying Checksum\n",
      "93f75153ba2d: Download complete\n",
      "57257bbf7ef3: Verifying Checksum\n",
      "57257bbf7ef3: Download complete\n",
      "69627a2639c6: Verifying Checksum\n",
      "69627a2639c6: Download complete\n",
      "5b3e203b7296: Verifying Checksum\n",
      "5b3e203b7296: Download complete\n",
      "6c5f0a7c49e2: Download complete\n",
      "fd3bdfa06834: Download complete\n",
      "e2d7418d62ad: Verifying Checksum\n",
      "e2d7418d62ad: Download complete\n",
      "22a53069998a: Pull complete\n",
      "db550b9db251: Pull complete\n",
      "10a826755d7e: Pull complete\n",
      "a35b01115e7d: Verifying Checksum\n",
      "a35b01115e7d: Download complete\n",
      "457c96476eac: Pull complete\n",
      "e2d7418d62ad: Pull complete\n",
      "a0d6fa61e0b7: Pull complete\n",
      "93f75153ba2d: Pull complete\n",
      "57257bbf7ef3: Pull complete\n",
      "69627a2639c6: Pull complete\n",
      "6c5f0a7c49e2: Pull complete\n",
      "5b3e203b7296: Pull complete\n",
      "a35b01115e7d: Pull complete\n",
      "fd3bdfa06834: Pull complete\n",
      "Digest: sha256:e6fc39b4b22cd1076a1f813448b6a05fd2d40fdc7dce57c62271d345960e975d\n",
      "Status: Downloaded newer image for gputraining0ccd60bf.azurecr.io/azureml/azureml_1b21012d99b68bea171596e10bcaefa4:latest\n",
      "49002742e7b15c5570b53a5088b10e27d9b77f25607dec1c7f19f1c75a623b65\n",
      "2020/07/12 00:54:57 Starting App Insight Logger for task:  containerSetup\n",
      "2020/07/12 00:54:57 Version: 3.0.01255.0021 Branch: JobPrepRelaseCustomDebug Commit: 34b9fb48\n",
      "2020/07/12 00:54:57 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2020/07/12 00:54:57 sshd inside container not required for job, skipping setup.\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "[2020-07-12T00:54:58.039223] Entering job preparation.\n",
      "[2020-07-12T00:54:58.685058] Starting job preparation.\n",
      "[2020-07-12T00:54:58.685092] Extracting the control code.\n",
      "[2020-07-12T00:54:58.919797] fetching and extracting the control code on master node.\n",
      "[2020-07-12T00:55:00.070679] Retrieving project from snapshot: 5c3c482d-b908-4616-94b8-8ab840745411\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 59\n",
      "[2020-07-12T00:55:00.071548] Start RetrieveProjectSasUrls\n",
      "[2020-07-12T00:55:00.193557] Finished RetrieveProjectSasUrls\n",
      "[2020-07-12T00:55:00.194576] Starting project file download.\n",
      "[2020-07-12T00:55:00.317690] Finished project file download.\n",
      "[2020-07-12T00:55:00.341611] Finished fetching and extracting the control code.\n",
      "[2020-07-12T00:55:00.344037] downloadDataStore - Download from datastores if requested.\n",
      "[2020-07-12T00:55:00.345011] Start run_history_prep.\n",
      "[2020-07-12T00:55:00.489636] Entering context manager injector.\n",
      "[2020-07-12T00:55:01.063705] downloadDataStore completed\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "[2020-07-12T00:55:09.934120] Entering context manager injector.\n",
      "Initialize DatasetContextManager.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 121\n",
      "Set Dataset mnist's target path to /tmp/tmp3wqddvub\n",
      "Enter __enter__ of DatasetContextManager\n",
      "SDK version: azureml-core==1.9.0 azureml-dataprep==1.9.1. Session id: f80fa5c5-9f88-4d14-8cfd-d13dc22c183d.\n",
      "Processing 'mnist'\n",
      "Processing dataset FileDataset\n",
      "{\n",
      "  \"source\": [\n",
      "    \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "    \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "    \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "    \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetFiles\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"7b573201-f261-434b-bb36-a102a6dcfe45\",\n",
      "    \"name\": \"mnist dataset\",\n",
      "    \"version\": 1,\n",
      "    \"description\": \"training and test dataset\",\n",
      "    \"workspace\": \"Workspace.create(name='gputraining', subscription_id='c46a9435-c957-4e6c-a0f4-b9a597984773', resource_group='mlops')\"\n",
      "  }\n",
      "}\n",
      "Mounting mnist to /tmp/tmp3wqddvub\n",
      "Mounted mnist to /tmp/tmp3wqddvub as folder.\n",
      "Exit __enter__ of DatasetContextManager\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ tf_mnist.py ] with arguments: ['--data-folder', '$mnist', '--batch-size', '50', '--first-layer-neurons', '300', '--second-layer-neurons', '100', '--learning-rate', '0.01']\n",
      "After variable expansion, calling script [ tf_mnist.py ] with arguments: ['--data-folder', '/tmp/tmp3wqddvub', '--batch-size', '50', '--first-layer-neurons', '300', '--second-layer-neurons', '100', '--learning-rate', '0.01']\n",
      "\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING - From tf_mnist.py:58: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING - From tf_mnist.py:112: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING - From tf_mnist.py:141: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "2020-07-12 00:55:49.409627: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-07-12 00:55:49.540007: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55ad50f48e10 executing computations on platform CUDA. Devices:\n",
      "2020-07-12 00:55:49.540088: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2020-07-12 00:55:49.542467: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2596990000 Hz\n",
      "2020-07-12 00:55:49.542855: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55ad510133b0 executing computations on platform Host. Devices:\n",
      "2020-07-12 00:55:49.542932: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2020-07-12 00:55:49.543099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: a9d1:00:00.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.11GiB\n",
      "2020-07-12 00:55:49.543157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2020-07-12 00:55:49.545546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-07-12 00:55:49.545604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2020-07-12 00:55:49.545643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2020-07-12 00:55:49.545763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10804 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: a9d1:00:00.0, compute capability: 3.7)\n",
      "2020-07-12 00:55:50.286984: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "Iter 1280, Minibatch Loss= 19748.001953, Training Accuracy= 0.23438\n",
      "Iter 2560, Minibatch Loss= 8073.717773, Training Accuracy= 0.53125\n",
      "Iter 3840, Minibatch Loss= 6981.041016, Training Accuracy= 0.66406\n",
      "Iter 5120, Minibatch Loss= 5106.124512, Training Accuracy= 0.71094\n",
      "Iter 6400, Minibatch Loss= 3548.612793, Training Accuracy= 0.76562\n",
      "Iter 7680, Minibatch Loss= 2585.050781, Training Accuracy= 0.83594\n",
      "Iter 8960, Minibatch Loss= 2285.970459, Training Accuracy= 0.86719\n",
      "Iter 10240, Minibatch Loss= 2773.346436, Training Accuracy= 0.84375\n",
      "Iter 11520, Minibatch Loss= 2597.785889, Training Accuracy= 0.82812\n",
      "Iter 12800, Minibatch Loss= 566.269836, Training Accuracy= 0.92969\n",
      "Iter 14080, Minibatch Loss= 2001.837158, Training Accuracy= 0.85156\n",
      "Iter 15360, Minibatch Loss= 1611.363281, Training Accuracy= 0.90625\n",
      "Iter 16640, Minibatch Loss= 1486.619629, Training Accuracy= 0.89844\n",
      "Iter 17920, Minibatch Loss= 1251.162598, Training Accuracy= 0.89844\n",
      "Iter 19200, Minibatch Loss= 1127.675049, Training Accuracy= 0.93750\n",
      "Iter 20480, Minibatch Loss= 519.437134, Training Accuracy= 0.92969\n",
      "Iter 21760, Minibatch Loss= 1785.470703, Training Accuracy= 0.89844\n",
      "Iter 23040, Minibatch Loss= 1510.412231, Training Accuracy= 0.86719\n",
      "Iter 24320, Minibatch Loss= 1230.844727, Training Accuracy= 0.92969\n",
      "Iter 25600, Minibatch Loss= 749.781616, Training Accuracy= 0.91406\n",
      "Iter 26880, Minibatch Loss= 1431.213257, Training Accuracy= 0.90625\n",
      "Iter 28160, Minibatch Loss= 651.726257, Training Accuracy= 0.92188\n",
      "Iter 29440, Minibatch Loss= 1058.746094, Training Accuracy= 0.93750\n",
      "Iter 30720, Minibatch Loss= 1063.538452, Training Accuracy= 0.90625\n",
      "Iter 32000, Minibatch Loss= 818.934875, Training Accuracy= 0.95312\n",
      "Iter 33280, Minibatch Loss= 303.077209, Training Accuracy= 0.95312\n",
      "Iter 34560, Minibatch Loss= 1077.255615, Training Accuracy= 0.95312\n",
      "Iter 35840, Minibatch Loss= 1464.832153, Training Accuracy= 0.93750\n",
      "Iter 37120, Minibatch Loss= 256.503113, Training Accuracy= 0.96875\n",
      "Iter 38400, Minibatch Loss= 1129.165527, Training Accuracy= 0.92188\n",
      "Iter 39680, Minibatch Loss= 390.025940, Training Accuracy= 0.94531\n",
      "Iter 40960, Minibatch Loss= 266.566650, Training Accuracy= 0.94531\n",
      "Iter 42240, Minibatch Loss= 1486.608887, Training Accuracy= 0.91406\n",
      "Iter 43520, Minibatch Loss= 816.142822, Training Accuracy= 0.94531\n",
      "Iter 44800, Minibatch Loss= 656.976929, Training Accuracy= 0.96094\n",
      "Iter 46080, Minibatch Loss= 434.839905, Training Accuracy= 0.96875\n",
      "Iter 47360, Minibatch Loss= 488.949799, Training Accuracy= 0.96094\n",
      "Iter 48640, Minibatch Loss= 413.337738, Training Accuracy= 0.94531\n",
      "Iter 49920, Minibatch Loss= 253.147919, Training Accuracy= 0.98438\n",
      "Iter 51200, Minibatch Loss= 1362.831543, Training Accuracy= 0.92969\n",
      "Iter 52480, Minibatch Loss= 468.563904, Training Accuracy= 0.95312\n",
      "Iter 53760, Minibatch Loss= 505.010010, Training Accuracy= 0.95312\n",
      "Iter 55040, Minibatch Loss= 267.632477, Training Accuracy= 0.96094\n",
      "Iter 56320, Minibatch Loss= 726.584229, Training Accuracy= 0.95312\n",
      "Iter 57600, Minibatch Loss= 419.357666, Training Accuracy= 0.96875\n",
      "Iter 58880, Minibatch Loss= 929.497192, Training Accuracy= 0.92969\n",
      "Iter 60160, Minibatch Loss= 94.581772, Training Accuracy= 0.99219\n",
      "Iter 61440, Minibatch Loss= 780.723145, Training Accuracy= 0.95312\n",
      "Iter 62720, Minibatch Loss= 480.119385, Training Accuracy= 0.96094\n",
      "Iter 64000, Minibatch Loss= 991.068970, Training Accuracy= 0.93750\n",
      "Iter 65280, Minibatch Loss= 672.967163, Training Accuracy= 0.95312\n",
      "Iter 66560, Minibatch Loss= 863.790527, Training Accuracy= 0.92188\n",
      "Iter 67840, Minibatch Loss= 1137.808472, Training Accuracy= 0.92969\n",
      "Iter 69120, Minibatch Loss= 733.318787, Training Accuracy= 0.95312\n",
      "Iter 70400, Minibatch Loss= 600.693909, Training Accuracy= 0.93750\n",
      "Iter 71680, Minibatch Loss= 607.574219, Training Accuracy= 0.95312\n",
      "Iter 72960, Minibatch Loss= 256.280487, Training Accuracy= 0.96875\n",
      "Iter 74240, Minibatch Loss= 386.015289, Training Accuracy= 0.93750\n",
      "Iter 75520, Minibatch Loss= 435.377472, Training Accuracy= 0.94531\n",
      "Iter 76800, Minibatch Loss= 124.366127, Training Accuracy= 0.96875\n",
      "Iter 78080, Minibatch Loss= 838.684326, Training Accuracy= 0.92969\n",
      "Iter 79360, Minibatch Loss= 138.370682, Training Accuracy= 0.96094\n",
      "Iter 80640, Minibatch Loss= 217.852570, Training Accuracy= 0.97656\n",
      "Iter 81920, Minibatch Loss= 336.988953, Training Accuracy= 0.94531\n",
      "Iter 83200, Minibatch Loss= 90.084305, Training Accuracy= 0.96094\n",
      "Iter 84480, Minibatch Loss= 182.646118, Training Accuracy= 0.97656\n",
      "Iter 85760, Minibatch Loss= 429.679932, Training Accuracy= 0.93750\n",
      "Iter 87040, Minibatch Loss= 222.763718, Training Accuracy= 0.96875\n",
      "Iter 88320, Minibatch Loss= 102.948532, Training Accuracy= 0.98438\n",
      "Iter 89600, Minibatch Loss= 599.982300, Training Accuracy= 0.93750\n",
      "Iter 90880, Minibatch Loss= 646.687012, Training Accuracy= 0.92969\n",
      "Iter 92160, Minibatch Loss= 608.605896, Training Accuracy= 0.92969\n",
      "Iter 93440, Minibatch Loss= 938.658142, Training Accuracy= 0.92969\n",
      "Iter 94720, Minibatch Loss= 720.092285, Training Accuracy= 0.91406\n",
      "Iter 96000, Minibatch Loss= 322.482208, Training Accuracy= 0.96094\n",
      "Iter 97280, Minibatch Loss= 102.082146, Training Accuracy= 0.96875\n",
      "Iter 98560, Minibatch Loss= 154.785950, Training Accuracy= 0.98438\n",
      "Iter 99840, Minibatch Loss= 426.440063, Training Accuracy= 0.97656\n",
      "Iter 101120, Minibatch Loss= 336.051025, Training Accuracy= 0.96875\n",
      "Iter 102400, Minibatch Loss= 140.726715, Training Accuracy= 0.96875\n",
      "Iter 103680, Minibatch Loss= 378.262146, Training Accuracy= 0.97656\n",
      "Iter 104960, Minibatch Loss= 61.012207, Training Accuracy= 0.98438\n",
      "Iter 106240, Minibatch Loss= 360.108368, Training Accuracy= 0.94531\n",
      "Iter 107520, Minibatch Loss= 229.927673, Training Accuracy= 0.97656\n",
      "Iter 108800, Minibatch Loss= 354.407013, Training Accuracy= 0.96875\n",
      "Iter 110080, Minibatch Loss= 101.378868, Training Accuracy= 0.98438\n",
      "Iter 111360, Minibatch Loss= 541.737549, Training Accuracy= 0.95312\n",
      "Iter 112640, Minibatch Loss= 91.199593, Training Accuracy= 0.99219\n",
      "Iter 113920, Minibatch Loss= 20.094276, Training Accuracy= 0.98438\n",
      "Iter 115200, Minibatch Loss= 101.501282, Training Accuracy= 0.96094\n",
      "Iter 116480, Minibatch Loss= 438.928802, Training Accuracy= 0.94531\n",
      "Iter 117760, Minibatch Loss= 44.446663, Training Accuracy= 0.97656\n",
      "Iter 119040, Minibatch Loss= 81.516243, Training Accuracy= 0.98438\n",
      "Iter 120320, Minibatch Loss= 271.804230, Training Accuracy= 0.96875\n",
      "Iter 121600, Minibatch Loss= 163.407059, Training Accuracy= 0.98438\n",
      "Iter 122880, Minibatch Loss= 154.761719, Training Accuracy= 0.98438\n",
      "Iter 124160, Minibatch Loss= 318.744812, Training Accuracy= 0.95312\n",
      "Iter 125440, Minibatch Loss= 196.994324, Training Accuracy= 0.97656\n",
      "Iter 126720, Minibatch Loss= 15.670059, Training Accuracy= 0.98438\n",
      "Iter 128000, Minibatch Loss= 158.170532, Training Accuracy= 0.97656\n",
      "Iter 129280, Minibatch Loss= 459.769653, Training Accuracy= 0.92969\n",
      "Iter 130560, Minibatch Loss= 145.284073, Training Accuracy= 0.96875\n",
      "Iter 131840, Minibatch Loss= 187.199448, Training Accuracy= 0.97656\n",
      "Iter 133120, Minibatch Loss= 195.993729, Training Accuracy= 0.96875\n",
      "Iter 134400, Minibatch Loss= 40.918236, Training Accuracy= 0.98438\n",
      "Iter 135680, Minibatch Loss= 262.237885, Training Accuracy= 0.96875\n",
      "Iter 136960, Minibatch Loss= 130.136383, Training Accuracy= 0.97656\n",
      "Iter 138240, Minibatch Loss= 111.193825, Training Accuracy= 0.99219\n",
      "Iter 139520, Minibatch Loss= 42.723244, Training Accuracy= 0.99219\n",
      "Iter 140800, Minibatch Loss= 191.839645, Training Accuracy= 0.97656\n",
      "Iter 142080, Minibatch Loss= 238.905441, Training Accuracy= 0.96875\n",
      "Iter 143360, Minibatch Loss= 278.955811, Training Accuracy= 0.93750\n",
      "Iter 144640, Minibatch Loss= 335.229675, Training Accuracy= 0.96875\n",
      "Iter 145920, Minibatch Loss= 133.070740, Training Accuracy= 0.95312\n",
      "Iter 147200, Minibatch Loss= 375.283813, Training Accuracy= 0.96875\n",
      "Iter 148480, Minibatch Loss= 252.235764, Training Accuracy= 0.97656\n",
      "Iter 149760, Minibatch Loss= 219.132294, Training Accuracy= 0.96875\n",
      "Iter 151040, Minibatch Loss= 247.252563, Training Accuracy= 0.96094\n",
      "Iter 152320, Minibatch Loss= 101.655365, Training Accuracy= 0.97656\n",
      "Iter 153600, Minibatch Loss= 461.462219, Training Accuracy= 0.92969\n",
      "Iter 154880, Minibatch Loss= 18.540405, Training Accuracy= 0.99219\n",
      "Iter 156160, Minibatch Loss= 401.160034, Training Accuracy= 0.94531\n",
      "Iter 157440, Minibatch Loss= 35.869202, Training Accuracy= 0.98438\n",
      "Iter 158720, Minibatch Loss= 231.848694, Training Accuracy= 0.97656\n",
      "Iter 160000, Minibatch Loss= 143.550629, Training Accuracy= 0.97656\n",
      "Iter 161280, Minibatch Loss= 248.721390, Training Accuracy= 0.96875\n",
      "Iter 162560, Minibatch Loss= 164.831436, Training Accuracy= 0.97656\n",
      "Iter 163840, Minibatch Loss= 221.422867, Training Accuracy= 0.98438\n",
      "Iter 165120, Minibatch Loss= 107.777336, Training Accuracy= 0.96875\n",
      "Iter 166400, Minibatch Loss= 16.886703, Training Accuracy= 0.98438\n",
      "Iter 167680, Minibatch Loss= 10.572147, Training Accuracy= 0.97656\n",
      "Iter 168960, Minibatch Loss= 54.235672, Training Accuracy= 0.97656\n",
      "Iter 170240, Minibatch Loss= 371.409546, Training Accuracy= 0.92969\n",
      "Iter 171520, Minibatch Loss= 436.565033, Training Accuracy= 0.96094\n",
      "Iter 172800, Minibatch Loss= 18.169388, Training Accuracy= 0.98438\n",
      "Iter 174080, Minibatch Loss= 22.558739, Training Accuracy= 0.99219\n",
      "Iter 175360, Minibatch Loss= 225.651459, Training Accuracy= 0.96094\n",
      "Iter 176640, Minibatch Loss= 175.301758, Training Accuracy= 0.96875\n",
      "Iter 177920, Minibatch Loss= 119.695717, Training Accuracy= 0.96094\n",
      "Iter 179200, Minibatch Loss= 142.649216, Training Accuracy= 0.97656\n",
      "Iter 180480, Minibatch Loss= 249.262817, Training Accuracy= 0.96875\n",
      "Iter 181760, Minibatch Loss= 93.034454, Training Accuracy= 0.96875\n",
      "Iter 183040, Minibatch Loss= 110.041763, Training Accuracy= 0.98438\n",
      "Iter 184320, Minibatch Loss= 59.211105, Training Accuracy= 0.98438\n",
      "Iter 185600, Minibatch Loss= 18.422348, Training Accuracy= 0.99219\n",
      "Iter 186880, Minibatch Loss= 339.761108, Training Accuracy= 0.96094\n",
      "Iter 188160, Minibatch Loss= 136.056564, Training Accuracy= 0.96094\n",
      "Iter 189440, Minibatch Loss= 278.731842, Training Accuracy= 0.93750\n",
      "Iter 190720, Minibatch Loss= 94.535942, Training Accuracy= 0.97656\n",
      "Iter 192000, Minibatch Loss= 81.820671, Training Accuracy= 0.96875\n",
      "Iter 193280, Minibatch Loss= 16.174278, Training Accuracy= 0.97656\n",
      "Iter 194560, Minibatch Loss= 212.896790, Training Accuracy= 0.97656\n",
      "Iter 195840, Minibatch Loss= 74.016525, Training Accuracy= 0.96875\n",
      "Iter 197120, Minibatch Loss= 198.675461, Training Accuracy= 0.96875\n",
      "Iter 198400, Minibatch Loss= 304.762390, Training Accuracy= 0.96875\n",
      "Iter 199680, Minibatch Loss= 21.009714, Training Accuracy= 0.97656\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9765625\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 121\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "3 items cleaning up...\n",
      "Exception encountered while calling history_context.__exit__ . Exception will be ignored to avoid a run failure because the step is optional.\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_restclient/workspace_client.py\", line 86, in _execute_with_arguments\n",
      "    return self._call_api(func, *args_list, **kwargs)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_restclient/clientbase.py\", line 232, in _call_api\n",
      "    return self._execute_with_base_arguments(func, *args, **kwargs)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_restclient/clientbase.py\", line 321, in _execute_with_base_arguments\n",
      "    back_off, total_retry, self._logger, func, _noop_reset, *args, **kwargs)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_restclient/clientbase.py\", line 336, in _execute_func_internal\n",
      "    left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_restclient/clientbase.py\", line 385, in _handle_retry\n",
      "    raise error\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_restclient/clientbase.py\", line 334, in _execute_func_internal\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_restclient/operations/metric_operations.py\", line 158, in post_run_metrics\n",
      "    raise models.ErrorResponseException(self._deserialize, response)\n",
      "azureml._restclient.models.error_response.ErrorResponseException: (ValidationError) Metric Document is too large, 4267b is larger than the max metric size 3000b.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/batch/tasks/shared/LS_root/jobs/gputraining/azureml/tf-mnist_1594515227_43c16e73/mounts/workspaceblobstore/azureml/tf-mnist_1594515227_43c16e73/azureml-setup/context_managers.py\", line 227, in __exit__\n",
      "    self.history_context.__exit__(*args)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_history/utils/context_managers.py\", line 56, in __exit__\n",
      "    return self._exit_stack.__exit__(*args)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/contextlib.py\", line 380, in __exit__\n",
      "    raise exc_details[1]\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/contextlib.py\", line 365, in __exit__\n",
      "    if cb(*exc_details):\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/contextlib.py\", line 284, in _exit_wrapper\n",
      "    return cm_exit(cm, *exc_details)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_history/utils/context_managers.py\", line 164, in __exit__\n",
      "    _RunBase._kill(timeout=self._kill_timeout)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_run_impl/run_base.py\", line 159, in _kill\n",
      "    handler(timeout)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_run_impl/run_base.py\", line 169, in _cleanup\n",
      "    self._client.flush(timeout)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_run_impl/run_history_facade.py\", line 620, in flush\n",
      "    self.metrics.flush(timeout_seconds=timeout_seconds)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_restclient/metrics_client.py\", line 271, in flush\n",
      "    self._task_queue_v2.flush(self.identity, timeout_seconds=timeout_seconds)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_common/async_utils/batch_task_queue.py\", line 89, in flush\n",
      "    super(BatchTaskQueue, self).flush(*args, **kwargs)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_common/async_utils/task_queue.py\", line 112, in flush\n",
      "    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_common/async_utils/task_queue.py\", line 112, in <genexpr>\n",
      "    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_common/async_utils/async_task.py\", line 59, in wait\n",
      "    res = self._handler(self._future, self._logger)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_common/async_utils/async_task.py\", line 16, in basic_handler\n",
      "    return future.result()\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/concurrent/futures/_base.py\", line 398, in result\n",
      "    return self.__get_result()\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/concurrent/futures/_base.py\", line 357, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/concurrent/futures/thread.py\", line 55, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_restclient/metrics_client.py\", line 337, in _log_batch_v2\n",
      "    is_async=is_async)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_restclient/run_client.py\", line 563, in _execute_with_workspace_run_arguments\n",
      "    return self._execute_with_arguments(func, workspace_run_arguments, *args, **kwargs)\n",
      "  File \"/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/azureml/_restclient/workspace_client.py\", line 88, in _execute_with_arguments\n",
      "    raise ServiceException(e)\n",
      "azureml._restclient.exceptions.ServiceException: ServiceException:\n",
      "\tCode: 400\n",
      "\tMessage: (ValidationError) Metric Document is too large, 4267b is larger than the max metric size 3000b.\n",
      "\tDetails:\n",
      "\t\tMetric Document is too large, 4267b is larger than the max metric size 3000b.\n",
      "\n",
      "\tHeaders: {\n",
      "\t    \"Date\": \"Sun, 12 Jul 2020 00:56:10 GMT\",\n",
      "\t    \"Content-Type\": \"application/json; charset=utf-8\",\n",
      "\t    \"Transfer-Encoding\": \"chunked\",\n",
      "\t    \"Connection\": \"keep-alive\",\n",
      "\t    \"Request-Context\": \"appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d\",\n",
      "\t    \"x-ms-response-type\": \"error\",\n",
      "\t    \"x-ms-client-request-id\": \"26cbb213-f155-4a9b-9984-28ec472bacf2\",\n",
      "\t    \"x-ms-client-session-id\": \"\",\n",
      "\t    \"Strict-Transport-Security\": \"max-age=15724800; includeSubDomains; preload\"\n",
      "\t}\n",
      "\tInnerException: {\n",
      "    \"additional_properties\": {\n",
      "        \"componentName\": \"run-history\"\n",
      "    },\n",
      "    \"error\": {\n",
      "        \"additional_properties\": {\n",
      "            \"severity\": null\n",
      "        },\n",
      "        \"code\": \"ValidationError\",\n",
      "        \"message\": \"Metric Document is too large, 4267b is larger than the max metric size 3000b.\",\n",
      "        \"details_uri\": null,\n",
      "        \"target\": \"MetricBatch\",\n",
      "        \"details\": [\n",
      "            {\n",
      "                \"additional_properties\": {\n",
      "                    \"severity\": null\n",
      "                },\n",
      "                \"code\": \"Invalid\",\n",
      "                \"message\": \"Metric Document is too large, 4267b is larger than the max metric size 3000b.\",\n",
      "                \"details_uri\": null,\n",
      "                \"target\": \"MetricBatch\",\n",
      "                \"details\": [],\n",
      "                \"inner_error\": null,\n",
      "                \"debug_info\": null,\n",
      "                \"message_format\": null,\n",
      "                \"message_parameters\": {},\n",
      "                \"reference_code\": null\n",
      "            }\n",
      "        ],\n",
      "        \"inner_error\": null,\n",
      "        \"debug_info\": null,\n",
      "        \"message_format\": null,\n",
      "        \"message_parameters\": null,\n",
      "        \"reference_code\": null\n",
      "    },\n",
      "    \"correlation\": {\n",
      "        \"operation\": \"dc37f0774a4d4a47aec602baf062d964\",\n",
      "        \"request\": \"e055a1c9d1784e72\"\n",
      "    },\n",
      "    \"environment\": \"westus2\",\n",
      "    \"location\": \"westus2\",\n",
      "    \"time\": {}\n",
      "}\n",
      "Enter __exit__ of DatasetContextManager\n",
      "Unmounting /tmp/tmp3wqddvub.\n",
      "Finishing unmounting /tmp/tmp3wqddvub.\n",
      "Exit __exit__ of DatasetContextManager\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "Entering job release. Current time:2020-07-12T00:56:20.338954\n",
      "Starting job release. Current time:2020-07-12T00:56:21.137346\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 478\n",
      "[2020-07-12T00:56:21.146863] Entering context manager injector.\n",
      "Job release is complete. Current time:2020-07-12T00:56:22.242043\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: tf-mnist_1594515227_43c16e73\n",
      "Web View: https://ml.azure.com/experiments/tf-mnist/runs/tf-mnist_1594515227_43c16e73?wsid=/subscriptions/c46a9435-c957-4e6c-a0f4-b9a597984773/resourcegroups/mlops/workspaces/gputraining\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'tf-mnist_1594515227_43c16e73',\n",
       " 'target': 'gpucluster1',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-07-12T00:54:11.090638Z',\n",
       " 'endTimeUtc': '2020-07-12T00:56:27.649946Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': '2d33e7cc-ace5-44d1-bef2-dd84f6fa206b',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
       " 'inputDatasets': [{'dataset': {'id': '7b573201-f261-434b-bb36-a102a6dcfe45'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'mnist', 'mechanism': 'Mount'}}],\n",
       " 'runDefinition': {'script': 'tf_mnist.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--data-folder',\n",
       "   'DatasetConsumptionConfig:mnist',\n",
       "   '--batch-size',\n",
       "   '50',\n",
       "   '--first-layer-neurons',\n",
       "   '300',\n",
       "   '--second-layer-neurons',\n",
       "   '100',\n",
       "   '--learning-rate',\n",
       "   '0.01'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'gpucluster1',\n",
       "  'dataReferences': {},\n",
       "  'data': {'mnist': {'dataLocation': {'dataset': {'id': '7b573201-f261-434b-bb36-a102a6dcfe45',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Mount',\n",
       "    'environmentVariableName': 'mnist',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'outputData': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment tf-mnist Environment',\n",
       "   'version': 'Autosave_2020-07-11T23:27:48Z_8f2c3c65',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-dataprep[pandas,fuse]',\n",
       "        'azureml-defaults',\n",
       "        'tensorflow-gpu==1.13.1',\n",
       "        'horovod==0.16.1']}],\n",
       "     'name': 'azureml_c0e081827c5441d25ef4420172ac8ce1'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'itpCompute': {'configuration': {}},\n",
       "  'cmAksCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594515227_43c16e73/azureml-logs/55_azureml-execution-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt?sv=2019-02-02&sr=b&sig=ssdUrdRubzEkRZd1hNZA2DjHt%2Fct%2BASr62MxzX5PqVI%3D&st=2020-07-12T00%3A46%3A28Z&se=2020-07-12T08%3A56%3A28Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594515227_43c16e73/azureml-logs/65_job_prep-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt?sv=2019-02-02&sr=b&sig=EQrccgK2c%2BowTIX5RdDzwV%2BSxi0hkl2IxXxkJceC8AU%3D&st=2020-07-12T00%3A46%3A28Z&se=2020-07-12T08%3A56%3A28Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594515227_43c16e73/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=%2B3JBqjps16HA4phOmyghLTdzgVjiesbodndict4F%2BH4%3D&st=2020-07-12T00%3A46%3A28Z&se=2020-07-12T08%3A56%3A28Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594515227_43c16e73/azureml-logs/75_job_post-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt?sv=2019-02-02&sr=b&sig=nQ6RPziroNUy6qOyCqe4QOqKdRv7%2FYCnEEoDW2KaPFw%3D&st=2020-07-12T00%3A46%3A28Z&se=2020-07-12T08%3A56%3A28Z&sp=r',\n",
       "  'azureml-logs/process_info.json': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594515227_43c16e73/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=X0Z4f4rRqgUvZNqgwDfks0LQjLuhmPLtniFFj8sFbR0%3D&st=2020-07-12T00%3A46%3A28Z&se=2020-07-12T08%3A56%3A28Z&sp=r',\n",
       "  'azureml-logs/process_status.json': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594515227_43c16e73/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=sn1t%2BPzxwqiZSw4lDPr%2FEvYN7n2v7UVMlanA%2F9m1Zao%3D&st=2020-07-12T00%3A46%3A28Z&se=2020-07-12T08%3A56%3A28Z&sp=r',\n",
       "  'logs/azureml/121_azureml.log': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594515227_43c16e73/logs/azureml/121_azureml.log?sv=2019-02-02&sr=b&sig=MQb4BgypVD4owpmfjGwYkcCTpYUcA3a9hapzcWEqGrM%3D&st=2020-07-12T00%3A46%3A28Z&se=2020-07-12T08%3A56%3A28Z&sp=r',\n",
       "  'logs/azureml/job_prep_azureml.log': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594515227_43c16e73/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=aI%2F9m7m9lOMyPAvNwPZcSTX6KDQ%2F7pFazwif3TgJz6U%3D&st=2020-07-12T00%3A46%3A28Z&se=2020-07-12T08%3A56%3A28Z&sp=r',\n",
       "  'logs/azureml/job_release_azureml.log': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594515227_43c16e73/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=S9SJnMk6KgascLcP1VBcGm0302hpx7ssSCmcPShGOyw%3D&st=2020-07-12T00%3A46%3A28Z&se=2020-07-12T08%3A56%3A28Z&sp=r'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelPathNotFoundException",
     "evalue": "ModelPathNotFoundException:\n\tMessage: Could not locate the provided model_path outputs/model in the set of files uploaded to the run: ['azureml-logs/55_azureml-execution-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt', 'azureml-logs/65_job_prep-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt', 'azureml-logs/70_driver_log.txt', 'azureml-logs/75_job_post-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt', 'azureml-logs/process_info.json', 'azureml-logs/process_status.json', 'logs/azureml/121_azureml.log', 'logs/azureml/job_prep_azureml.log', 'logs/azureml/job_release_azureml.log']\n                See https://aka.ms/run-logging for more details.\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Could not locate the provided model_path outputs/model in the set of files uploaded to the run: ['azureml-logs/55_azureml-execution-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt', 'azureml-logs/65_job_prep-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt', 'azureml-logs/70_driver_log.txt', 'azureml-logs/75_job_post-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt', 'azureml-logs/process_info.json', 'azureml-logs/process_status.json', 'logs/azureml/121_azureml.log', 'logs/azureml/job_prep_azureml.log', 'logs/azureml/job_release_azureml.log']\\n                See https://aka.ms/run-logging for more details.\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelPathNotFoundException\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7027463d9bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                            \u001b[0mmodel_framework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFramework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTENSORFLOW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                            \u001b[0mmodel_framework_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1.13.0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                            resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5))\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/run.py\u001b[0m in \u001b[0;36mregister_model\u001b[0;34m(self, model_name, model_path, tags, properties, model_framework, model_framework_version, description, datasets, sample_input_dataset, sample_output_dataset, resource_configuration, **kwargs)\u001b[0m\n\u001b[1;32m   2012\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_framework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_framework_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_input_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_input_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2014\u001b[0;31m             sample_output_dataset=sample_output_dataset, resource_configuration=resource_configuration, **kwargs)\n\u001b[0m\u001b[1;32m   2015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_dataset_lineage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/_run_impl/run_history_facade.py\u001b[0m in \u001b[0;36mregister_model\u001b[0;34m(self, model_name, model_path, tags, properties, model_framework, model_framework_version, asset_id, sample_input_dataset, sample_output_dataset, resource_configuration, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m             raise ModelPathNotFoundException(\n\u001b[1;32m    401\u001b[0m                 \"\"\"Could not locate the provided model_path {} in the set of files uploaded to the run: {}\n\u001b[0;32m--> 402\u001b[0;31m                 See https://aka.ms/run-logging for more details.\"\"\".format(model_path, str(run_files)))\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0martifacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"prefix\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0martifact_prefix_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mmetadata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelPathNotFoundException\u001b[0m: ModelPathNotFoundException:\n\tMessage: Could not locate the provided model_path outputs/model in the set of files uploaded to the run: ['azureml-logs/55_azureml-execution-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt', 'azureml-logs/65_job_prep-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt', 'azureml-logs/70_driver_log.txt', 'azureml-logs/75_job_post-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt', 'azureml-logs/process_info.json', 'azureml-logs/process_status.json', 'logs/azureml/121_azureml.log', 'logs/azureml/job_prep_azureml.log', 'logs/azureml/job_release_azureml.log']\n                See https://aka.ms/run-logging for more details.\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Could not locate the provided model_path outputs/model in the set of files uploaded to the run: ['azureml-logs/55_azureml-execution-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt', 'azureml-logs/65_job_prep-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt', 'azureml-logs/70_driver_log.txt', 'azureml-logs/75_job_post-tvmps_c4bca20bd952bede855b6ce84989bd025280fbbc42ced880e9fd39a96792d523_d.txt', 'azureml-logs/process_info.json', 'azureml-logs/process_status.json', 'logs/azureml/121_azureml.log', 'logs/azureml/job_prep_azureml.log', 'logs/azureml/job_release_azureml.log']\\n                See https://aka.ms/run-logging for more details.\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "model = run.register_model(model_name='tf-dnn-mnist', \n",
    "                           model_path='outputs/model',\n",
    "                           model_framework=Model.Framework.TENSORFLOW,\n",
    "                           model_framework_version='1.13.0',\n",
    "                           resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model folder in the current directory\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    if f.startswith('outputs/model'):\n",
    "        output_file_path = os.path.join('./model', f.split('/')[-1])\n",
    "        print('Downloading from {} to {} ...'.format(f, output_file_path))\n",
    "        run.download_file(name=f, output_file_path=output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "# Tensorflow constructor\n",
    "estimator= TensorFlow(source_directory=project_folder,\n",
    "                      compute_target=compute_target,\n",
    "                      script_params=script_params,\n",
    "                      entry_script='script.py',\n",
    "                      node_count=2,\n",
    "                      process_count_per_node=1,\n",
    "                      distributed_training=MpiConfiguration(),\n",
    "                      framework_version='1.13',\n",
    "                      use_gpu=True,\n",
    "                      pip_packages=['azureml-dataprep[pandas,fuse]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "distributed_training = TensorflowConfiguration()\n",
    "distributed_training.worker_count = 2\n",
    "\n",
    "# Tensorflow constructor\n",
    "tf_est= TensorFlow(source_directory=project_folder,\n",
    "                      compute_target=compute_target,\n",
    "                      script_params=script_params,\n",
    "                      entry_script='script.py',\n",
    "                      node_count=2,\n",
    "                      process_count_per_node=1,\n",
    "                      distributed_training=distributed_training,\n",
    "                      use_gpu=True,\n",
    "                      pip_packages=['azureml-dataprep[pandas,fuse]'])\n",
    "\n",
    "# submit the TensorFlow job\n",
    "run = exp.submit(tf_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_CONFIG='{\n",
    "    \"cluster\": {\n",
    "        \"ps\": [\"host0:2222\", \"host1:2222\"],\n",
    "        \"worker\": [\"host2:2222\", \"host3:2222\", \"host4:2222\"],\n",
    "    },\n",
    "    \"task\": {\"type\": \"ps\", \"index\": 0},\n",
    "    \"environment\": \"cloud\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_config = os.environ.get('TF_CONFIG')\n",
    "if not tf_config or tf_config == \"\":\n",
    "    raise ValueError(\"TF_CONFIG not found.\")\n",
    "tf_config_json = json.loads(tf_config)\n",
    "cluster_spec = tf.train.ClusterSpec(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Model.deploy(ws, \"tensorflow-web-service\", [model])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
