{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import shutil\n",
    "import azureml\n",
    "\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace, Run\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.dnn import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-20.1.1-py2.py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.0.2\n",
      "    Uninstalling pip-20.0.2:\n",
      "      Successfully uninstalled pip-20.0.2\n",
      "Successfully installed pip-20.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2.0\n",
      "  Downloading tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |█████████████████████           | 338.3 MB 91.9 MB/s eta 0:00:024     |███████▊                        | 125.0 MB 110.9 MB/s eta 0:00:04     |███████████████                 | 242.4 MB 96.2 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 516.2 MB 7.0 kB/s s eta 0:00:01��█████████████████████  | 483.7 MB 108.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 42.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.29.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (3.12.1)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 45.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (3.2.1)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (2.2.1)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (0.9.0)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow==2.2.0) (46.4.0.post20200518)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.6.0.post3)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.16.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
      "\u001b[31mERROR: tensorflow-gpu 2.1.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-gpu 2.1.0 has requirement tensorboard<2.2.0,>=2.1.0, but you'll have tensorboard 2.2.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-gpu 2.1.0 has requirement tensorflow-estimator<2.2.0,>=2.1.0rc0, but you'll have tensorflow-estimator 2.2.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: h5py, tensorflow-estimator, gast, astunparse, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.9.0\n",
      "    Uninstalling h5py-2.9.0:\n",
      "      Successfully uninstalled h5py-2.9.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.2.2\n",
      "    Uninstalling gast-0.2.2:\n",
      "      Successfully uninstalled gast-0.2.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.1.0\n",
      "    Uninstalling tensorflow-2.1.0:\n",
      "      Successfully uninstalled tensorflow-2.1.0\n",
      "Successfully installed astunparse-1.6.3 gast-0.3.3 h5py-2.10.0 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement azureml.logging (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for azureml.logging\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install azureml.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    " \n",
    "svc_pr_password = \"1fY58u0dpP1Yg-i.A~rUp_iz04RxWUFSwv\"\n",
    " \n",
    "svc_pr = ServicePrincipalAuthentication(\n",
    "    tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\",\n",
    "    service_principal_id=\"8a3ddafe-6dd6-48af-867e-d745232a1833\",\n",
    "    service_principal_password=\"1fY58u0dpP1Yg-i.A~rUp_iz04RxWUFSwv\")\n",
    " \n",
    "ws = Workspace(\n",
    "    subscription_id=\"c46a9435-c957-4e6c-a0f4-b9a597984773\",\n",
    "    resource_group=\"mlops\",\n",
    "    workspace_name=\"gputraining\",\n",
    "    auth=svc_pr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='gputraining', subscription_id='c46a9435-c957-4e6c-a0f4-b9a597984773', resource_group='mlops')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = './tf-mnist'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "exp = Experiment(workspace=ws, name='tf-mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "web_paths = [\n",
    "            'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "            'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "            'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "            'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "            ]\n",
    "dataset = Dataset.File.from_files(path=web_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/http/yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
       " '/http/yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
       " '/http/yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
       " '/http/yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.register(workspace=ws,\n",
    "                           name='mnist dataset',\n",
    "                           description='training and test dataset',\n",
    "                           create_new_version=True)\n",
    "\n",
    "# list the files referenced by dataset\n",
    "dataset.to_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n"
     ]
    }
   ],
   "source": [
    "cluster_name = \"gpucluster1\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - framework_version is not specified, defaulting to version 1.13.\n",
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['azureml-dataprep']. We cannot guarantee image build will succeed.\n"
     ]
    }
   ],
   "source": [
    "script_params = {\n",
    "    '--data-folder': dataset.as_named_input('mnist').as_mount(),\n",
    "    '--batch-size': 50,\n",
    "    '--first-layer-neurons': 300,\n",
    "    '--second-layer-neurons': 100,\n",
    "    '--learning-rate': 0.01\n",
    "}\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 entry_script='tf_mnist.py',\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 use_gpu=True,\n",
    "                 pip_packages=['azureml-dataprep[pandas,fuse]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: tf-mnist_1594665871_65661143\n",
      "Web View: https://ml.azure.com/experiments/tf-mnist/runs/tf-mnist_1594665871_65661143?wsid=/subscriptions/c46a9435-c957-4e6c-a0f4-b9a597984773/resourcegroups/mlops/workspaces/gputraining\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_03e3d4df5cc6625032f5e60dc713068ae9ea78c44a4406a996dc251134b86c02_d.txt\n",
      "========================================================================================================================\n",
      "\n",
      "2020-07-13T18:48:05Z Executing 'Copy ACR Details file' on 10.0.0.4\n",
      "2020-07-13T18:48:05Z Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "2020-07-13T18:48:05Z Starting output-watcher...\n",
      "2020-07-13T18:48:05Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_1b21012d99b68bea171596e10bcaefa4\n",
      "f7277927d38a: Pulling fs layer\n",
      "8d3eac894db4: Pulling fs layer\n",
      "edf72af6d627: Pulling fs layer\n",
      "3e4f86211d23: Pulling fs layer\n",
      "d6e9603ff777: Pulling fs layer\n",
      "5cad422780e2: Pulling fs layer\n",
      "8130687c8acb: Pulling fs layer\n",
      "c11e9246d621: Pulling fs layer\n",
      "0dfae24cbbd9: Pulling fs layer\n",
      "0bb049a6d391: Pulling fs layer\n",
      "22a53069998a: Pulling fs layer\n",
      "db550b9db251: Pulling fs layer\n",
      "10a826755d7e: Pulling fs layer\n",
      "457c96476eac: Pulling fs layer\n",
      "e2d7418d62ad: Pulling fs layer\n",
      "a0d6fa61e0b7: Pulling fs layer\n",
      "93f75153ba2d: Pulling fs layer\n",
      "3e4f86211d23: Waiting\n",
      "57257bbf7ef3: Pulling fs layer\n",
      "69627a2639c6: Pulling fs layer\n",
      "d6e9603ff777: Waiting\n",
      "6c5f0a7c49e2: Pulling fs layer\n",
      "5b3e203b7296: Pulling fs layer\n",
      "5cad422780e2: Waiting\n",
      "a35b01115e7d: Pulling fs layer\n",
      "8130687c8acb: Waiting\n",
      "fd3bdfa06834: Pulling fs layer\n",
      "c11e9246d621: Waiting\n",
      "0dfae24cbbd9: Waiting\n",
      "0bb049a6d391: Waiting\n",
      "22a53069998a: Waiting\n",
      "57257bbf7ef3: Waiting\n",
      "69627a2639c6: Waiting\n",
      "db550b9db251: Waiting\n",
      "a35b01115e7d: Waiting\n",
      "6c5f0a7c49e2: Waiting\n",
      "10a826755d7e: Waiting\n",
      "5b3e203b7296: Waiting\n",
      "a0d6fa61e0b7: Waiting\n",
      "457c96476eac: Waiting\n",
      "93f75153ba2d: Waiting\n",
      "fd3bdfa06834: Waiting\n",
      "edf72af6d627: Verifying Checksum\n",
      "edf72af6d627: Download complete\n",
      "8d3eac894db4: Download complete\n",
      "3e4f86211d23: Verifying Checksum\n",
      "3e4f86211d23: Download complete\n",
      "d6e9603ff777: Verifying Checksum\n",
      "d6e9603ff777: Download complete\n",
      "8130687c8acb: Verifying Checksum\n",
      "8130687c8acb: Download complete\n",
      "f7277927d38a: Verifying Checksum\n",
      "f7277927d38a: Download complete\n",
      "5cad422780e2: Verifying Checksum\n",
      "5cad422780e2: Download complete\n",
      "f7277927d38a: Pull complete\n",
      "8d3eac894db4: Pull complete\n",
      "edf72af6d627: Pull complete\n",
      "3e4f86211d23: Pull complete\n",
      "d6e9603ff777: Pull complete\n",
      "0bb049a6d391: Verifying Checksum\n",
      "0bb049a6d391: Download complete\n",
      "5cad422780e2: Pull complete\n",
      "c11e9246d621: Verifying Checksum\n",
      "c11e9246d621: Download complete\n",
      "8130687c8acb: Pull complete\n",
      "db550b9db251: Verifying Checksum\n",
      "db550b9db251: Download complete\n",
      "22a53069998a: Verifying Checksum\n",
      "22a53069998a: Download complete\n",
      "10a826755d7e: Verifying Checksum\n",
      "10a826755d7e: Download complete\n",
      "457c96476eac: Verifying Checksum\n",
      "457c96476eac: Download complete\n",
      "0dfae24cbbd9: Verifying Checksum\n",
      "0dfae24cbbd9: Download complete\n",
      "a0d6fa61e0b7: Verifying Checksum\n",
      "a0d6fa61e0b7: Download complete\n",
      "57257bbf7ef3: Verifying Checksum\n",
      "57257bbf7ef3: Download complete\n",
      "93f75153ba2d: Download complete\n",
      "69627a2639c6: Verifying Checksum\n",
      "69627a2639c6: Download complete\n",
      "5b3e203b7296: Verifying Checksum\n",
      "5b3e203b7296: Download complete\n",
      "6c5f0a7c49e2: Verifying Checksum\n",
      "6c5f0a7c49e2: Download complete\n",
      "e2d7418d62ad: Verifying Checksum\n",
      "e2d7418d62ad: Download complete\n",
      "fd3bdfa06834: Verifying Checksum\n",
      "fd3bdfa06834: Download complete\n",
      "a35b01115e7d: Verifying Checksum\n",
      "a35b01115e7d: Download complete\n",
      "c11e9246d621: Pull complete\n",
      "0dfae24cbbd9: Pull complete\n",
      "0bb049a6d391: Pull complete\n",
      "22a53069998a: Pull complete\n",
      "db550b9db251: Pull complete\n",
      "10a826755d7e: Pull complete\n",
      "457c96476eac: Pull complete\n",
      "e2d7418d62ad: Pull complete\n",
      "a0d6fa61e0b7: Pull complete\n",
      "93f75153ba2d: Pull complete\n",
      "57257bbf7ef3: Pull complete\n",
      "69627a2639c6: Pull complete\n",
      "6c5f0a7c49e2: Pull complete\n",
      "5b3e203b7296: Pull complete\n",
      "a35b01115e7d: Pull complete\n",
      "fd3bdfa06834: Pull complete\n",
      "Digest: sha256:e6fc39b4b22cd1076a1f813448b6a05fd2d40fdc7dce57c62271d345960e975d\n",
      "Status: Downloaded newer image for gputraining0ccd60bf.azurecr.io/azureml/azureml_1b21012d99b68bea171596e10bcaefa4:latest\n",
      "d9a58e55268930c4b35494d4fa7c6cf5127ab6c3679a14ce391adcb801573595\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_03e3d4df5cc6625032f5e60dc713068ae9ea78c44a4406a996dc251134b86c02_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "[2020-07-13T18:49:59.435751] Entering job preparation.\n",
      "[2020-07-13T18:50:00.514578] Starting job preparation.\n",
      "[2020-07-13T18:50:00.514614] Extracting the control code.\n",
      "[2020-07-13T18:50:00.547170] fetching and extracting the control code on master node.\n",
      "[2020-07-13T18:50:01.918049] Retrieving project from snapshot: d392b5c7-b9b7-41d2-bd2b-08dcb4516701\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 58\n",
      "[2020-07-13T18:50:01.919310] Start RetrieveProjectSasUrls\n",
      "[2020-07-13T18:50:02.386167] Finished RetrieveProjectSasUrls\n",
      "[2020-07-13T18:50:02.387804] Starting project file download.\n",
      "[2020-07-13T18:50:02.652765] Finished project file download.\n",
      "[2020-07-13T18:50:02.668497] Finished fetching and extracting the control code.\n",
      "[2020-07-13T18:50:02.670990] downloadDataStore - Download from datastores if requested.\n",
      "[2020-07-13T18:50:02.671845] Start run_history_prep.\n",
      "[2020-07-13T18:50:02.726769] Entering context manager injector.\n",
      "[2020-07-13T18:50:03.311163] downloadDataStore completed\n",
      "[2020-07-13T18:50:03.313862] Job preparation is complete.\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "[2020-07-13T18:50:04.319622] Entering context manager injector.\n",
      "Initialize DatasetContextManager.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 121\n",
      "Set Dataset mnist's target path to /tmp/tmp66shhwaa\n",
      "Enter __enter__ of DatasetContextManager\n",
      "SDK version: azureml-core==1.9.0 azureml-dataprep==1.9.1. Session id: 4a725f02-7d62-4ae0-a13d-7708a8ce06d2.\n",
      "Processing 'mnist'\n",
      "Processing dataset FileDataset\n",
      "{\n",
      "  \"source\": [\n",
      "    \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "    \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "    \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "    \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetFiles\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"7b573201-f261-434b-bb36-a102a6dcfe45\",\n",
      "    \"name\": \"mnist dataset\",\n",
      "    \"version\": 1,\n",
      "    \"description\": \"training and test dataset\",\n",
      "    \"workspace\": \"Workspace.create(name='gputraining', subscription_id='c46a9435-c957-4e6c-a0f4-b9a597984773', resource_group='mlops')\"\n",
      "  }\n",
      "}\n",
      "Mounting mnist to /tmp/tmp66shhwaa\n",
      "Mounted mnist to /tmp/tmp66shhwaa as folder.\n",
      "Exit __enter__ of DatasetContextManager\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ tf_mnist.py ] with arguments: ['--data-folder', '$mnist', '--batch-size', '50', '--first-layer-neurons', '300', '--second-layer-neurons', '100', '--learning-rate', '0.01']\n",
      "After variable expansion, calling script [ tf_mnist.py ] with arguments: ['--data-folder', '/tmp/tmp66shhwaa', '--batch-size', '50', '--first-layer-neurons', '300', '--second-layer-neurons', '100', '--learning-rate', '0.01']\n",
      "\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING - From tf_mnist.py:66: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING - From tf_mnist.py:120: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING - From tf_mnist.py:149: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "2020-07-13 18:50:35.358235: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-07-13 18:50:35.697639: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5642eae4c120 executing computations on platform CUDA. Devices:\n",
      "2020-07-13 18:50:35.697715: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2020-07-13 18:50:35.700047: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2596990000 Hz\n",
      "2020-07-13 18:50:35.700468: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5642eaf168a0 executing computations on platform Host. Devices:\n",
      "2020-07-13 18:50:35.700539: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2020-07-13 18:50:35.700841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: f193:00:00.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.11GiB\n",
      "2020-07-13 18:50:35.700903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2020-07-13 18:50:35.703297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-07-13 18:50:35.703351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2020-07-13 18:50:35.703391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2020-07-13 18:50:35.703493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10804 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: f193:00:00.0, compute capability: 3.7)\n",
      "2020-07-13 18:50:36.450405: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "Iter 1280, Minibatch Loss= 19753.990234, Training Accuracy= 0.28906\n",
      "Iter 2560, Minibatch Loss= 9244.139648, Training Accuracy= 0.61719\n",
      "Iter 3840, Minibatch Loss= 8312.024414, Training Accuracy= 0.62500\n",
      "Iter 5120, Minibatch Loss= 3789.815918, Training Accuracy= 0.79688\n",
      "Iter 6400, Minibatch Loss= 3454.304443, Training Accuracy= 0.78906\n",
      "Iter 7680, Minibatch Loss= 4902.503906, Training Accuracy= 0.78125\n",
      "Iter 8960, Minibatch Loss= 4236.154297, Training Accuracy= 0.80469\n",
      "Iter 10240, Minibatch Loss= 2437.263184, Training Accuracy= 0.85156\n",
      "Iter 11520, Minibatch Loss= 2027.497070, Training Accuracy= 0.85156\n",
      "Iter 12800, Minibatch Loss= 4024.544434, Training Accuracy= 0.84375\n",
      "Iter 14080, Minibatch Loss= 3393.481445, Training Accuracy= 0.85156\n",
      "Iter 15360, Minibatch Loss= 2198.904541, Training Accuracy= 0.83594\n",
      "Iter 16640, Minibatch Loss= 1472.411621, Training Accuracy= 0.89844\n",
      "Iter 17920, Minibatch Loss= 1339.580322, Training Accuracy= 0.90625\n",
      "Iter 19200, Minibatch Loss= 627.868835, Training Accuracy= 0.93750\n",
      "Iter 20480, Minibatch Loss= 1089.305908, Training Accuracy= 0.92188\n",
      "Iter 21760, Minibatch Loss= 901.389343, Training Accuracy= 0.92969\n",
      "Iter 23040, Minibatch Loss= 736.812439, Training Accuracy= 0.92969\n",
      "Iter 24320, Minibatch Loss= 958.764160, Training Accuracy= 0.91406\n",
      "Iter 25600, Minibatch Loss= 968.701721, Training Accuracy= 0.91406\n",
      "Iter 26880, Minibatch Loss= 936.607788, Training Accuracy= 0.94531\n",
      "Iter 28160, Minibatch Loss= 1109.276001, Training Accuracy= 0.88281\n",
      "Iter 29440, Minibatch Loss= 1137.693848, Training Accuracy= 0.91406\n",
      "Iter 30720, Minibatch Loss= 1551.342773, Training Accuracy= 0.90625\n",
      "Iter 32000, Minibatch Loss= 905.872437, Training Accuracy= 0.93750\n",
      "Iter 33280, Minibatch Loss= 632.308411, Training Accuracy= 0.95312\n",
      "Iter 34560, Minibatch Loss= 419.874329, Training Accuracy= 0.94531\n",
      "Iter 35840, Minibatch Loss= 376.386658, Training Accuracy= 0.95312\n",
      "Iter 37120, Minibatch Loss= 1127.849121, Training Accuracy= 0.90625\n",
      "Iter 38400, Minibatch Loss= 2264.535156, Training Accuracy= 0.91406\n",
      "Iter 39680, Minibatch Loss= 1018.811646, Training Accuracy= 0.96094\n",
      "Iter 40960, Minibatch Loss= 622.098633, Training Accuracy= 0.96875\n",
      "Iter 42240, Minibatch Loss= 777.124451, Training Accuracy= 0.92188\n",
      "Iter 43520, Minibatch Loss= 803.783752, Training Accuracy= 0.92188\n",
      "Iter 44800, Minibatch Loss= 1183.351196, Training Accuracy= 0.91406\n",
      "Iter 46080, Minibatch Loss= 898.762207, Training Accuracy= 0.92969\n",
      "Iter 47360, Minibatch Loss= 536.946533, Training Accuracy= 0.92969\n",
      "Iter 48640, Minibatch Loss= 476.815491, Training Accuracy= 0.94531\n",
      "Iter 49920, Minibatch Loss= 234.927902, Training Accuracy= 0.97656\n",
      "Iter 51200, Minibatch Loss= 702.393982, Training Accuracy= 0.94531\n",
      "Iter 52480, Minibatch Loss= 351.054382, Training Accuracy= 0.94531\n",
      "Iter 53760, Minibatch Loss= 749.696106, Training Accuracy= 0.92188\n",
      "Iter 55040, Minibatch Loss= 490.070190, Training Accuracy= 0.96875\n",
      "Iter 56320, Minibatch Loss= 1054.428223, Training Accuracy= 0.95312\n",
      "Iter 57600, Minibatch Loss= 636.769714, Training Accuracy= 0.94531\n",
      "Iter 58880, Minibatch Loss= 921.654785, Training Accuracy= 0.93750\n",
      "Iter 60160, Minibatch Loss= 739.039307, Training Accuracy= 0.93750\n",
      "Iter 61440, Minibatch Loss= 1089.440186, Training Accuracy= 0.92969\n",
      "Iter 62720, Minibatch Loss= 195.916443, Training Accuracy= 0.94531\n",
      "Iter 64000, Minibatch Loss= 395.180420, Training Accuracy= 0.96875\n",
      "Iter 65280, Minibatch Loss= 530.465088, Training Accuracy= 0.96875\n",
      "Iter 66560, Minibatch Loss= 149.041168, Training Accuracy= 0.99219\n",
      "Iter 67840, Minibatch Loss= 271.717651, Training Accuracy= 0.96875\n",
      "Iter 69120, Minibatch Loss= 683.599426, Training Accuracy= 0.92188\n",
      "Iter 70400, Minibatch Loss= 146.289978, Training Accuracy= 0.97656\n",
      "Iter 71680, Minibatch Loss= 758.371643, Training Accuracy= 0.94531\n",
      "Iter 72960, Minibatch Loss= 192.799881, Training Accuracy= 0.97656\n",
      "Iter 74240, Minibatch Loss= 1591.954834, Training Accuracy= 0.91406\n",
      "Iter 75520, Minibatch Loss= 451.543335, Training Accuracy= 0.93750\n",
      "Iter 76800, Minibatch Loss= 33.633369, Training Accuracy= 0.96875\n",
      "Iter 78080, Minibatch Loss= 152.138031, Training Accuracy= 0.97656\n",
      "Iter 79360, Minibatch Loss= 280.342163, Training Accuracy= 0.97656\n",
      "Iter 80640, Minibatch Loss= 1070.136230, Training Accuracy= 0.92969\n",
      "Iter 81920, Minibatch Loss= 822.020935, Training Accuracy= 0.94531\n",
      "Iter 83200, Minibatch Loss= 1291.238159, Training Accuracy= 0.93750\n",
      "Iter 84480, Minibatch Loss= 231.282242, Training Accuracy= 0.98438\n",
      "Iter 85760, Minibatch Loss= 9.568039, Training Accuracy= 0.99219\n",
      "Iter 87040, Minibatch Loss= 69.938919, Training Accuracy= 0.98438\n",
      "Iter 88320, Minibatch Loss= 619.851929, Training Accuracy= 0.95312\n",
      "Iter 89600, Minibatch Loss= 390.107483, Training Accuracy= 0.96875\n",
      "Iter 90880, Minibatch Loss= 653.495117, Training Accuracy= 0.93750\n",
      "Iter 92160, Minibatch Loss= 65.677979, Training Accuracy= 0.97656\n",
      "Iter 93440, Minibatch Loss= 444.064941, Training Accuracy= 0.96875\n",
      "Iter 94720, Minibatch Loss= 459.542419, Training Accuracy= 0.96875\n",
      "Iter 96000, Minibatch Loss= 14.476852, Training Accuracy= 0.99219\n",
      "Iter 97280, Minibatch Loss= 545.996277, Training Accuracy= 0.95312\n",
      "Iter 98560, Minibatch Loss= 542.935791, Training Accuracy= 0.93750\n",
      "Iter 99840, Minibatch Loss= 417.314545, Training Accuracy= 0.96094\n",
      "Iter 101120, Minibatch Loss= 335.861542, Training Accuracy= 0.97656\n",
      "Iter 102400, Minibatch Loss= 327.847870, Training Accuracy= 0.96094\n",
      "Iter 103680, Minibatch Loss= 281.085571, Training Accuracy= 0.94531\n",
      "Iter 104960, Minibatch Loss= 90.752411, Training Accuracy= 0.96094\n",
      "Iter 106240, Minibatch Loss= 399.101685, Training Accuracy= 0.96875\n",
      "Iter 107520, Minibatch Loss= 228.750458, Training Accuracy= 0.96875\n",
      "Iter 108800, Minibatch Loss= 518.528503, Training Accuracy= 0.95312\n",
      "Iter 110080, Minibatch Loss= 229.573990, Training Accuracy= 0.96094\n",
      "Iter 111360, Minibatch Loss= 305.000183, Training Accuracy= 0.98438\n",
      "Iter 112640, Minibatch Loss= 272.005859, Training Accuracy= 0.96094\n",
      "Iter 113920, Minibatch Loss= 404.786835, Training Accuracy= 0.96875\n",
      "Iter 115200, Minibatch Loss= 164.439362, Training Accuracy= 0.97656\n",
      "Iter 116480, Minibatch Loss= 314.841553, Training Accuracy= 0.99219\n",
      "Iter 117760, Minibatch Loss= 232.312637, Training Accuracy= 0.96094\n",
      "Iter 119040, Minibatch Loss= 75.661469, Training Accuracy= 0.98438\n",
      "Iter 120320, Minibatch Loss= 276.360168, Training Accuracy= 0.94531\n",
      "Iter 121600, Minibatch Loss= 176.394531, Training Accuracy= 0.96094\n",
      "Iter 122880, Minibatch Loss= 195.269226, Training Accuracy= 0.96875\n",
      "Iter 124160, Minibatch Loss= 353.147705, Training Accuracy= 0.96875\n",
      "Iter 125440, Minibatch Loss= 279.063690, Training Accuracy= 0.94531\n",
      "Iter 126720, Minibatch Loss= 302.747864, Training Accuracy= 0.94531\n",
      "Iter 128000, Minibatch Loss= 400.767517, Training Accuracy= 0.93750\n",
      "Iter 129280, Minibatch Loss= 748.986877, Training Accuracy= 0.92969\n",
      "Iter 130560, Minibatch Loss= 210.697510, Training Accuracy= 0.95312\n",
      "Iter 131840, Minibatch Loss= 123.950928, Training Accuracy= 0.97656\n",
      "Iter 133120, Minibatch Loss= 214.251236, Training Accuracy= 0.94531\n",
      "Iter 134400, Minibatch Loss= 393.356201, Training Accuracy= 0.93750\n",
      "Iter 135680, Minibatch Loss= 191.628815, Training Accuracy= 0.96875\n",
      "Iter 136960, Minibatch Loss= 106.899239, Training Accuracy= 0.97656\n",
      "Iter 138240, Minibatch Loss= 91.870872, Training Accuracy= 0.97656\n",
      "Iter 139520, Minibatch Loss= 91.741173, Training Accuracy= 0.98438\n",
      "Iter 140800, Minibatch Loss= 135.411285, Training Accuracy= 0.96875\n",
      "Iter 142080, Minibatch Loss= 228.044159, Training Accuracy= 0.96875\n",
      "Iter 143360, Minibatch Loss= 20.185577, Training Accuracy= 0.99219\n",
      "Iter 144640, Minibatch Loss= 231.342010, Training Accuracy= 0.96875\n",
      "Iter 145920, Minibatch Loss= 200.224106, Training Accuracy= 0.98438\n",
      "Iter 147200, Minibatch Loss= 233.599579, Training Accuracy= 0.95312\n",
      "Iter 148480, Minibatch Loss= 184.909103, Training Accuracy= 0.97656\n",
      "Iter 149760, Minibatch Loss= 69.623146, Training Accuracy= 0.96094\n",
      "Iter 151040, Minibatch Loss= 349.332153, Training Accuracy= 0.96875\n",
      "Iter 152320, Minibatch Loss= 74.784416, Training Accuracy= 0.97656\n",
      "Iter 153600, Minibatch Loss= 145.546295, Training Accuracy= 0.97656\n",
      "Iter 154880, Minibatch Loss= 424.262085, Training Accuracy= 0.96875\n",
      "Iter 156160, Minibatch Loss= 371.140778, Training Accuracy= 0.94531\n",
      "Iter 157440, Minibatch Loss= 223.797302, Training Accuracy= 0.96094\n",
      "Iter 158720, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 160000, Minibatch Loss= 412.330383, Training Accuracy= 0.92969\n",
      "Iter 161280, Minibatch Loss= 269.110565, Training Accuracy= 0.96094\n",
      "Iter 162560, Minibatch Loss= 131.650650, Training Accuracy= 0.96094\n",
      "Iter 163840, Minibatch Loss= 156.842697, Training Accuracy= 0.98438\n",
      "Iter 165120, Minibatch Loss= 87.676018, Training Accuracy= 0.98438\n",
      "Iter 166400, Minibatch Loss= 126.076942, Training Accuracy= 0.97656\n",
      "Iter 167680, Minibatch Loss= 106.269669, Training Accuracy= 0.97656\n",
      "Iter 168960, Minibatch Loss= 122.651741, Training Accuracy= 0.97656\n",
      "Iter 170240, Minibatch Loss= 102.389343, Training Accuracy= 0.98438\n",
      "Iter 171520, Minibatch Loss= 218.608322, Training Accuracy= 0.96875\n",
      "Iter 172800, Minibatch Loss= 9.631889, Training Accuracy= 0.99219\n",
      "Iter 174080, Minibatch Loss= 48.255486, Training Accuracy= 0.98438\n",
      "Iter 175360, Minibatch Loss= 75.692307, Training Accuracy= 0.98438\n",
      "Iter 176640, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 177920, Minibatch Loss= 64.614143, Training Accuracy= 0.98438\n",
      "Iter 179200, Minibatch Loss= 41.250237, Training Accuracy= 0.98438\n",
      "Iter 180480, Minibatch Loss= 166.717133, Training Accuracy= 0.96094\n",
      "Iter 181760, Minibatch Loss= 33.256248, Training Accuracy= 0.98438\n",
      "Iter 183040, Minibatch Loss= 176.803177, Training Accuracy= 0.96875\n",
      "Iter 184320, Minibatch Loss= 364.073486, Training Accuracy= 0.97656\n",
      "Iter 185600, Minibatch Loss= 175.582596, Training Accuracy= 0.96094\n",
      "Iter 186880, Minibatch Loss= 112.370468, Training Accuracy= 0.97656\n",
      "Iter 188160, Minibatch Loss= 176.615631, Training Accuracy= 0.96875\n",
      "Iter 189440, Minibatch Loss= 184.700211, Training Accuracy= 0.96875\n",
      "Iter 190720, Minibatch Loss= 181.685974, Training Accuracy= 0.98438\n",
      "Iter 192000, Minibatch Loss= 36.441101, Training Accuracy= 0.98438\n",
      "Iter 193280, Minibatch Loss= 240.172607, Training Accuracy= 0.98438\n",
      "Iter 194560, Minibatch Loss= 50.490524, Training Accuracy= 0.98438\n",
      "Iter 195840, Minibatch Loss= 101.737305, Training Accuracy= 0.97656\n",
      "Iter 197120, Minibatch Loss= 62.008888, Training Accuracy= 0.99219\n",
      "Iter 198400, Minibatch Loss= 253.153320, Training Accuracy= 0.95312\n",
      "Iter 199680, Minibatch Loss= 133.704346, Training Accuracy= 0.97656\n",
      "Optimization Finished!\n",
      "Length Accuracy: 156\n",
      "Length Losses: 156\n",
      "Testing Accuracy: 0.98828125\n",
      "Accuracy Avg: 0.98828\n",
      "Saved Model\n",
      "Read model and Save to Local\n",
      "Completed Write\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 121\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "3 items cleaning up...\n",
      "Cleanup took 1.4725847244262695 seconds\n",
      "Enter __exit__ of DatasetContextManager\n",
      "Unmounting /tmp/tmp66shhwaa.\n",
      "Finishing unmounting /tmp/tmp66shhwaa.\n",
      "Exit __exit__ of DatasetContextManager\n",
      "2020/07/13 18:51:08 Process Exiting with Code:  0\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_03e3d4df5cc6625032f5e60dc713068ae9ea78c44a4406a996dc251134b86c02_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "Entering job release. Current time:2020-07-13T18:51:08.517463\n",
      "Starting job release. Current time:2020-07-13T18:51:09.320479\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 487\n",
      "[2020-07-13T18:51:09.329795] Entering context manager injector.\n",
      "Job release is complete. Current time:2020-07-13T18:51:10.974765\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: tf-mnist_1594665871_65661143\n",
      "Web View: https://ml.azure.com/experiments/tf-mnist/runs/tf-mnist_1594665871_65661143?wsid=/subscriptions/c46a9435-c957-4e6c-a0f4-b9a597984773/resourcegroups/mlops/workspaces/gputraining\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'tf-mnist_1594665871_65661143',\n",
       " 'target': 'gpucluster1',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-07-13T18:48:00.675446Z',\n",
       " 'endTimeUtc': '2020-07-13T18:51:21.08525Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': 'c9f99d1e-9f4f-45dd-9c64-f6e0ad9e5e55',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
       " 'inputDatasets': [{'dataset': {'id': '7b573201-f261-434b-bb36-a102a6dcfe45'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'mnist', 'mechanism': 'Mount'}}],\n",
       " 'runDefinition': {'script': 'tf_mnist.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--data-folder',\n",
       "   'DatasetConsumptionConfig:mnist',\n",
       "   '--batch-size',\n",
       "   '50',\n",
       "   '--first-layer-neurons',\n",
       "   '300',\n",
       "   '--second-layer-neurons',\n",
       "   '100',\n",
       "   '--learning-rate',\n",
       "   '0.01'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'gpucluster1',\n",
       "  'dataReferences': {},\n",
       "  'data': {'mnist': {'dataLocation': {'dataset': {'id': '7b573201-f261-434b-bb36-a102a6dcfe45',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Mount',\n",
       "    'environmentVariableName': 'mnist',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'outputData': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment tf-mnist Environment',\n",
       "   'version': 'Autosave_2020-07-11T23:27:48Z_8f2c3c65',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-dataprep[pandas,fuse]',\n",
       "        'azureml-defaults',\n",
       "        'tensorflow-gpu==1.13.1',\n",
       "        'horovod==0.16.1']}],\n",
       "     'name': 'azureml_c0e081827c5441d25ef4420172ac8ce1'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'itpCompute': {'configuration': {}},\n",
       "  'cmAksCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_03e3d4df5cc6625032f5e60dc713068ae9ea78c44a4406a996dc251134b86c02_d.txt': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594665871_65661143/azureml-logs/55_azureml-execution-tvmps_03e3d4df5cc6625032f5e60dc713068ae9ea78c44a4406a996dc251134b86c02_d.txt?sv=2019-02-02&sr=b&sig=R2f9FpI0rrkoIfa1kVlPd4CirGCzf%2FrTQVeyPiva4VY%3D&st=2020-07-13T18%3A41%3A22Z&se=2020-07-14T02%3A51%3A22Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_03e3d4df5cc6625032f5e60dc713068ae9ea78c44a4406a996dc251134b86c02_d.txt': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594665871_65661143/azureml-logs/65_job_prep-tvmps_03e3d4df5cc6625032f5e60dc713068ae9ea78c44a4406a996dc251134b86c02_d.txt?sv=2019-02-02&sr=b&sig=p6A6jlASVwQH28Mz9UOje7%2FN0S45PPN9ixE%2BVGQhYss%3D&st=2020-07-13T18%3A41%3A22Z&se=2020-07-14T02%3A51%3A22Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594665871_65661143/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=SVWiyGIkk03oP1AVEoV441N1i2lMr6oB3Oj6PU3c7QY%3D&st=2020-07-13T18%3A41%3A22Z&se=2020-07-14T02%3A51%3A22Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_03e3d4df5cc6625032f5e60dc713068ae9ea78c44a4406a996dc251134b86c02_d.txt': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594665871_65661143/azureml-logs/75_job_post-tvmps_03e3d4df5cc6625032f5e60dc713068ae9ea78c44a4406a996dc251134b86c02_d.txt?sv=2019-02-02&sr=b&sig=XzfHWhLk1qUWwcOTpBkn5qHMeDZErD1SYKI2b9%2F4GGM%3D&st=2020-07-13T18%3A41%3A22Z&se=2020-07-14T02%3A51%3A22Z&sp=r',\n",
       "  'azureml-logs/process_info.json': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594665871_65661143/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=oHVabbWRDjf2znEEhWbcn7FLO0vnG6u6aD8IaMoqvvE%3D&st=2020-07-13T18%3A41%3A22Z&se=2020-07-14T02%3A51%3A22Z&sp=r',\n",
       "  'azureml-logs/process_status.json': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594665871_65661143/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=P1oxxS7O6dpaVW6oqvYrfujkIEsRSl%2FX8OGxxqLUbGA%3D&st=2020-07-13T18%3A41%3A22Z&se=2020-07-14T02%3A51%3A22Z&sp=r',\n",
       "  'logs/azureml/121_azureml.log': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594665871_65661143/logs/azureml/121_azureml.log?sv=2019-02-02&sr=b&sig=BUAslpw3SlZNAFPMLJnJSiYxx6OgXiyhqWgmfqWJnmI%3D&st=2020-07-13T18%3A41%3A22Z&se=2020-07-14T02%3A51%3A22Z&sp=r',\n",
       "  'logs/azureml/job_prep_azureml.log': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594665871_65661143/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=Xkqib%2BCHwOVjfaWeUd19EUBimp1VqdQVQk6%2FqIymIew%3D&st=2020-07-13T18%3A41%3A22Z&se=2020-07-14T02%3A51%3A22Z&sp=r',\n",
       "  'logs/azureml/job_release_azureml.log': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594665871_65661143/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=qr7iW07RCfi8a2SyPho7LeMM1muFX7gdyVIqmJSV2Wk%3D&st=2020-07-13T18%3A41%3A22Z&se=2020-07-14T02%3A51%3A22Z&sp=r'}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azureml-logs/55_azureml-execution-tvmps_03e3d4df5cc6625032f5e60dc713068ae9ea78c44a4406a996dc251134b86c02_d.txt', 'azureml-logs/65_job_prep-tvmps_03e3d4df5cc6625032f5e60dc713068ae9ea78c44a4406a996dc251134b86c02_d.txt', 'azureml-logs/70_driver_log.txt', 'azureml-logs/75_job_post-tvmps_03e3d4df5cc6625032f5e60dc713068ae9ea78c44a4406a996dc251134b86c02_d.txt', 'azureml-logs/process_info.json', 'azureml-logs/process_status.json', 'logs/azureml/121_azureml.log', 'logs/azureml/job_prep_azureml.log', 'logs/azureml/job_release_azureml.log', 'outputs/model/checkpoint', 'outputs/model/tf-dnn-mnist.data-00000-of-00001', 'outputs/model/tf-dnn-mnist.index', 'outputs/model/tf-dnn-mnist.meta']\n"
     ]
    }
   ],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs('./outputs', exist_ok=True)\n",
    "#os.makedirs('./outputs/model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "model = run.register_model(model_name='tf-dnn-mnist', \n",
    "                           model_path='outputs/model',\n",
    "                           model_framework=Model.Framework.TENSORFLOW,\n",
    "                           model_framework_version='1.13.0',\n",
    "                           resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model folder in the current directory\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    if f.startswith('outputs/model'):\n",
    "        output_file_path = os.path.join('./model', f.split('/')[-1])\n",
    "        print('Downloading from {} to {} ...'.format(f, output_file_path))\n",
    "        run.download_file(name=f, output_file_path=output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "# Tensorflow constructor\n",
    "estimator= TensorFlow(source_directory=project_folder,\n",
    "                      compute_target=compute_target,\n",
    "                      script_params=script_params,\n",
    "                      entry_script='script.py',\n",
    "                      node_count=2,\n",
    "                      process_count_per_node=1,\n",
    "                      distributed_training=MpiConfiguration(),\n",
    "                      framework_version='1.13',\n",
    "                      use_gpu=True,\n",
    "                      pip_packages=['azureml-dataprep[pandas,fuse]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "distributed_training = TensorflowConfiguration()\n",
    "distributed_training.worker_count = 2\n",
    "\n",
    "# Tensorflow constructor\n",
    "tf_est= TensorFlow(source_directory=project_folder,\n",
    "                      compute_target=compute_target,\n",
    "                      script_params=script_params,\n",
    "                      entry_script='script.py',\n",
    "                      node_count=2,\n",
    "                      process_count_per_node=1,\n",
    "                      distributed_training=distributed_training,\n",
    "                      use_gpu=True,\n",
    "                      pip_packages=['azureml-dataprep[pandas,fuse]'])\n",
    "\n",
    "# submit the TensorFlow job\n",
    "run = exp.submit(tf_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_CONFIG='{\n",
    "    \"cluster\": {\n",
    "        \"ps\": [\"host0:2222\", \"host1:2222\"],\n",
    "        \"worker\": [\"host2:2222\", \"host3:2222\", \"host4:2222\"],\n",
    "    },\n",
    "    \"task\": {\"type\": \"ps\", \"index\": 0},\n",
    "    \"environment\": \"cloud\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_config = os.environ.get('TF_CONFIG')\n",
    "if not tf_config or tf_config == \"\":\n",
    "    raise ValueError(\"TF_CONFIG not found.\")\n",
    "tf_config_json = json.loads(tf_config)\n",
    "cluster_spec = tf.train.ClusterSpec(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Model.deploy(ws, \"tensorflow-web-service\", [model])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
