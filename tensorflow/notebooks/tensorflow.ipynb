{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import shutil\n",
    "import azureml\n",
    "\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace, Run\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.dnn import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-20.1.1-py2.py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.0.2\n",
      "    Uninstalling pip-20.0.2:\n",
      "      Successfully uninstalled pip-20.0.2\n",
      "Successfully installed pip-20.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2.0\n",
      "  Downloading tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |█████████████████████           | 338.3 MB 91.9 MB/s eta 0:00:024     |███████▊                        | 125.0 MB 110.9 MB/s eta 0:00:04     |███████████████                 | 242.4 MB 96.2 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 516.2 MB 7.0 kB/s s eta 0:00:01��█████████████████████  | 483.7 MB 108.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 42.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.29.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (3.12.1)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 45.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (3.2.1)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (2.2.1)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (0.9.0)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow==2.2.0) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow==2.2.0) (46.4.0.post20200518)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.6.0.post3)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.16.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
      "\u001b[31mERROR: tensorflow-gpu 2.1.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-gpu 2.1.0 has requirement tensorboard<2.2.0,>=2.1.0, but you'll have tensorboard 2.2.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-gpu 2.1.0 has requirement tensorflow-estimator<2.2.0,>=2.1.0rc0, but you'll have tensorflow-estimator 2.2.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: h5py, tensorflow-estimator, gast, astunparse, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.9.0\n",
      "    Uninstalling h5py-2.9.0:\n",
      "      Successfully uninstalled h5py-2.9.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.2.2\n",
      "    Uninstalling gast-0.2.2:\n",
      "      Successfully uninstalled gast-0.2.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.1.0\n",
      "    Uninstalling tensorflow-2.1.0:\n",
      "      Successfully uninstalled tensorflow-2.1.0\n",
      "Successfully installed astunparse-1.6.3 gast-0.3.3 h5py-2.10.0 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement azureml.logging (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for azureml.logging\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install azureml.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    " \n",
    "svc_pr_password = \"1fY58u0dpP1Yg-i.A~rUp_iz04RxWUFSwv\"\n",
    " \n",
    "svc_pr = ServicePrincipalAuthentication(\n",
    "    tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\",\n",
    "    service_principal_id=\"8a3ddafe-6dd6-48af-867e-d745232a1833\",\n",
    "    service_principal_password=\"1fY58u0dpP1Yg-i.A~rUp_iz04RxWUFSwv\")\n",
    " \n",
    "ws = Workspace(\n",
    "    subscription_id=\"c46a9435-c957-4e6c-a0f4-b9a597984773\",\n",
    "    resource_group=\"mlops\",\n",
    "    workspace_name=\"gputraining\",\n",
    "    auth=svc_pr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='gputraining', subscription_id='c46a9435-c957-4e6c-a0f4-b9a597984773', resource_group='mlops')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = './tf-mnist'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "exp = Experiment(workspace=ws, name='tf-mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "web_paths = [\n",
    "            'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "            'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "            'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "            'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "            ]\n",
    "dataset = Dataset.File.from_files(path=web_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/http/yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
       " '/http/yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
       " '/http/yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
       " '/http/yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.register(workspace=ws,\n",
    "                           name='mnist dataset',\n",
    "                           description='training and test dataset',\n",
    "                           create_new_version=True)\n",
    "\n",
    "# list the files referenced by dataset\n",
    "dataset.to_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n"
     ]
    }
   ],
   "source": [
    "cluster_name = \"gpucluster1\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - framework_version is not specified, defaulting to version 1.13.\n",
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['azureml-dataprep']. We cannot guarantee image build will succeed.\n"
     ]
    }
   ],
   "source": [
    "script_params = {\n",
    "    '--data-folder': dataset.as_named_input('mnist').as_mount(),\n",
    "    '--batch-size': 50,\n",
    "    '--first-layer-neurons': 300,\n",
    "    '--second-layer-neurons': 100,\n",
    "    '--learning-rate': 0.01\n",
    "}\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 entry_script='tf_mnist.py',\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 use_gpu=True,\n",
    "                 pip_packages=['azureml-dataprep[pandas,fuse]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: tf-mnist_1594656213_4e742f4a\n",
      "Web View: https://ml.azure.com/experiments/tf-mnist/runs/tf-mnist_1594656213_4e742f4a?wsid=/subscriptions/c46a9435-c957-4e6c-a0f4-b9a597984773/resourcegroups/mlops/workspaces/gputraining\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_f7c54ae2e9b21df2aa5e81d647adb3413487cf8ff87a3e9909807b4608c61e90_d.txt\n",
      "========================================================================================================================\n",
      "\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_1b21012d99b68bea171596e10bcaefa4\n",
      "f7277927d38a: Pulling fs layer\n",
      "8d3eac894db4: Pulling fs layer\n",
      "edf72af6d627: Pulling fs layer\n",
      "3e4f86211d23: Pulling fs layer\n",
      "d6e9603ff777: Pulling fs layer\n",
      "5cad422780e2: Pulling fs layer\n",
      "8130687c8acb: Pulling fs layer\n",
      "c11e9246d621: Pulling fs layer\n",
      "0dfae24cbbd9: Pulling fs layer\n",
      "0bb049a6d391: Pulling fs layer\n",
      "22a53069998a: Pulling fs layer\n",
      "db550b9db251: Pulling fs layer\n",
      "10a826755d7e: Pulling fs layer\n",
      "457c96476eac: Pulling fs layer\n",
      "e2d7418d62ad: Pulling fs layer\n",
      "a0d6fa61e0b7: Pulling fs layer\n",
      "93f75153ba2d: Pulling fs layer\n",
      "57257bbf7ef3: Pulling fs layer\n",
      "69627a2639c6: Pulling fs layer\n",
      "6c5f0a7c49e2: Pulling fs layer\n",
      "5b3e203b7296: Pulling fs layer\n",
      "a35b01115e7d: Pulling fs layer\n",
      "fd3bdfa06834: Pulling fs layer\n",
      "db550b9db251: Waiting\n",
      "10a826755d7e: Waiting\n",
      "457c96476eac: Waiting\n",
      "e2d7418d62ad: Waiting\n",
      "8130687c8acb: Waiting\n",
      "c11e9246d621: Waiting\n",
      "5cad422780e2: Waiting\n",
      "d6e9603ff777: Waiting\n",
      "5b3e203b7296: Waiting\n",
      "0bb049a6d391: Waiting\n",
      "22a53069998a: Waiting\n",
      "fd3bdfa06834: Waiting\n",
      "69627a2639c6: Waiting\n",
      "93f75153ba2d: Waiting\n",
      "57257bbf7ef3: Waiting\n",
      "0dfae24cbbd9: Waiting\n",
      "6c5f0a7c49e2: Waiting\n",
      "a35b01115e7d: Waiting\n",
      "edf72af6d627: Verifying Checksum\n",
      "edf72af6d627: Download complete\n",
      "8d3eac894db4: Verifying Checksum\n",
      "8d3eac894db4: Download complete\n",
      "3e4f86211d23: Verifying Checksum\n",
      "3e4f86211d23: Download complete\n",
      "d6e9603ff777: Verifying Checksum\n",
      "d6e9603ff777: Download complete\n",
      "f7277927d38a: Verifying Checksum\n",
      "f7277927d38a: Download complete\n",
      "8130687c8acb: Verifying Checksum\n",
      "8130687c8acb: Download complete\n",
      "5cad422780e2: Download complete\n",
      "f7277927d38a: Pull complete\n",
      "8d3eac894db4: Pull complete\n",
      "edf72af6d627: Pull complete\n",
      "3e4f86211d23: Pull complete\n",
      "d6e9603ff777: Pull complete\n",
      "5cad422780e2: Pull complete\n",
      "8130687c8acb: Pull complete\n",
      "c11e9246d621: Download complete\n",
      "0bb049a6d391: Verifying Checksum\n",
      "0bb049a6d391: Download complete\n",
      "22a53069998a: Verifying Checksum\n",
      "22a53069998a: Download complete\n",
      "db550b9db251: Verifying Checksum\n",
      "db550b9db251: Download complete\n",
      "10a826755d7e: Verifying Checksum\n",
      "10a826755d7e: Download complete\n",
      "457c96476eac: Verifying Checksum\n",
      "457c96476eac: Download complete\n",
      "a0d6fa61e0b7: Verifying Checksum\n",
      "a0d6fa61e0b7: Download complete\n",
      "93f75153ba2d: Verifying Checksum\n",
      "93f75153ba2d: Download complete\n",
      "57257bbf7ef3: Verifying Checksum\n",
      "57257bbf7ef3: Download complete\n",
      "69627a2639c6: Verifying Checksum\n",
      "69627a2639c6: Download complete\n",
      "0dfae24cbbd9: Verifying Checksum\n",
      "0dfae24cbbd9: Download complete\n",
      "e2d7418d62ad: Verifying Checksum\n",
      "e2d7418d62ad: Download complete\n",
      "5b3e203b7296: Verifying Checksum\n",
      "5b3e203b7296: Download complete\n",
      "6c5f0a7c49e2: Download complete\n",
      "fd3bdfa06834: Download complete\n",
      "a35b01115e7d: Verifying Checksum\n",
      "a35b01115e7d: Download complete\n",
      "c11e9246d621: Pull complete\n",
      "0dfae24cbbd9: Pull complete\n",
      "0bb049a6d391: Pull complete\n",
      "22a53069998a: Pull complete\n",
      "db550b9db251: Pull complete\n",
      "10a826755d7e: Pull complete\n",
      "457c96476eac: Pull complete\n",
      "e2d7418d62ad: Pull complete\n",
      "a0d6fa61e0b7: Pull complete\n",
      "93f75153ba2d: Pull complete\n",
      "57257bbf7ef3: Pull complete\n",
      "69627a2639c6: Pull complete\n",
      "6c5f0a7c49e2: Pull complete\n",
      "5b3e203b7296: Pull complete\n",
      "a35b01115e7d: Pull complete\n",
      "fd3bdfa06834: Pull complete\n",
      "Digest: sha256:e6fc39b4b22cd1076a1f813448b6a05fd2d40fdc7dce57c62271d345960e975d\n",
      "Status: Downloaded newer image for gputraining0ccd60bf.azurecr.io/azureml/azureml_1b21012d99b68bea171596e10bcaefa4:latest\n",
      "70c0d3f3208bdc12954ff6db52e03aebd131cd9f1381c24fe731f20e76eed0a0\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_f7c54ae2e9b21df2aa5e81d647adb3413487cf8ff87a3e9909807b4608c61e90_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "[2020-07-13T16:07:56.511739] Entering job preparation.\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "[2020-07-13T16:08:00.698473] Entering context manager injector.\n",
      "Initialize DatasetContextManager.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 123\n",
      "Set Dataset mnist's target path to /tmp/tmpz_482mhr\n",
      "Enter __enter__ of DatasetContextManager\n",
      "SDK version: azureml-core==1.9.0 azureml-dataprep==1.9.1. Session id: b8477847-12b4-49d9-b306-e426f6a7f546.\n",
      "Processing 'mnist'\n",
      "Processing dataset FileDataset\n",
      "{\n",
      "  \"source\": [\n",
      "    \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
      "    \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
      "    \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "    \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetFiles\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"7b573201-f261-434b-bb36-a102a6dcfe45\",\n",
      "    \"name\": \"mnist dataset\",\n",
      "    \"version\": 1,\n",
      "    \"description\": \"training and test dataset\",\n",
      "    \"workspace\": \"Workspace.create(name='gputraining', subscription_id='c46a9435-c957-4e6c-a0f4-b9a597984773', resource_group='mlops')\"\n",
      "  }\n",
      "}\n",
      "Mounting mnist to /tmp/tmpz_482mhr\n",
      "Mounted mnist to /tmp/tmpz_482mhr as folder.\n",
      "Exit __enter__ of DatasetContextManager\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ tf_mnist.py ] with arguments: ['--data-folder', '$mnist', '--batch-size', '50', '--first-layer-neurons', '300', '--second-layer-neurons', '100', '--learning-rate', '0.01']\n",
      "After variable expansion, calling script [ tf_mnist.py ] with arguments: ['--data-folder', '/tmp/tmpz_482mhr', '--batch-size', '50', '--first-layer-neurons', '300', '--second-layer-neurons', '100', '--learning-rate', '0.01']\n",
      "\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING - From tf_mnist.py:65: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING - From /azureml-envs/azureml_c0e081827c5441d25ef4420172ac8ce1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING - From tf_mnist.py:119: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING - From tf_mnist.py:148: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "2020-07-13 16:08:35.317471: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-07-13 16:08:35.546397: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55b04df7ee70 executing computations on platform CUDA. Devices:\n",
      "2020-07-13 16:08:35.546480: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2020-07-13 16:08:35.548880: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2596990000 Hz\n",
      "2020-07-13 16:08:35.549450: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55b04e049420 executing computations on platform Host. Devices:\n",
      "2020-07-13 16:08:35.549524: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2020-07-13 16:08:35.549910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 4d8b:00:00.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.11GiB\n",
      "2020-07-13 16:08:35.549978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2020-07-13 16:08:35.552409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-07-13 16:08:35.552467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2020-07-13 16:08:35.552535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2020-07-13 16:08:35.552646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10804 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 4d8b:00:00.0, compute capability: 3.7)\n",
      "2020-07-13 16:08:36.292447: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "Iter 1280, Minibatch Loss= 29538.734375, Training Accuracy= 0.14844\n",
      "Iter 2560, Minibatch Loss= 11144.711914, Training Accuracy= 0.39062\n",
      "Iter 3840, Minibatch Loss= 6570.771484, Training Accuracy= 0.63281\n",
      "Iter 5120, Minibatch Loss= 4814.793457, Training Accuracy= 0.74219\n",
      "Iter 6400, Minibatch Loss= 1990.419434, Training Accuracy= 0.82812\n",
      "Iter 7680, Minibatch Loss= 3420.419434, Training Accuracy= 0.77344\n",
      "Iter 8960, Minibatch Loss= 3188.769775, Training Accuracy= 0.82031\n",
      "Iter 10240, Minibatch Loss= 2642.589600, Training Accuracy= 0.79688\n",
      "Iter 11520, Minibatch Loss= 2746.932129, Training Accuracy= 0.83594\n",
      "Iter 12800, Minibatch Loss= 2535.615234, Training Accuracy= 0.85938\n",
      "Iter 14080, Minibatch Loss= 1026.372314, Training Accuracy= 0.92188\n",
      "Iter 15360, Minibatch Loss= 1765.433228, Training Accuracy= 0.89844\n",
      "Iter 16640, Minibatch Loss= 2134.424072, Training Accuracy= 0.88281\n",
      "Iter 17920, Minibatch Loss= 1243.932617, Training Accuracy= 0.87500\n",
      "Iter 19200, Minibatch Loss= 951.658203, Training Accuracy= 0.88281\n",
      "Iter 20480, Minibatch Loss= 647.526978, Training Accuracy= 0.91406\n",
      "Iter 21760, Minibatch Loss= 1126.567749, Training Accuracy= 0.92969\n",
      "Iter 23040, Minibatch Loss= 861.937561, Training Accuracy= 0.92188\n",
      "Iter 24320, Minibatch Loss= 1215.154907, Training Accuracy= 0.88281\n",
      "Iter 25600, Minibatch Loss= 418.498383, Training Accuracy= 0.93750\n",
      "Iter 26880, Minibatch Loss= 992.955566, Training Accuracy= 0.92969\n",
      "Iter 28160, Minibatch Loss= 533.218201, Training Accuracy= 0.93750\n",
      "Iter 29440, Minibatch Loss= 670.078491, Training Accuracy= 0.93750\n",
      "Iter 30720, Minibatch Loss= 1661.044556, Training Accuracy= 0.90625\n",
      "Iter 32000, Minibatch Loss= 848.457458, Training Accuracy= 0.92969\n",
      "Iter 33280, Minibatch Loss= 981.005005, Training Accuracy= 0.92969\n",
      "Iter 34560, Minibatch Loss= 917.338135, Training Accuracy= 0.92188\n",
      "Iter 35840, Minibatch Loss= 936.713501, Training Accuracy= 0.93750\n",
      "Iter 37120, Minibatch Loss= 629.218018, Training Accuracy= 0.93750\n",
      "Iter 38400, Minibatch Loss= 1315.086670, Training Accuracy= 0.92969\n",
      "Iter 39680, Minibatch Loss= 509.153839, Training Accuracy= 0.94531\n",
      "Iter 40960, Minibatch Loss= 1084.622803, Training Accuracy= 0.91406\n",
      "Iter 42240, Minibatch Loss= 945.002197, Training Accuracy= 0.96094\n",
      "Iter 43520, Minibatch Loss= 499.275330, Training Accuracy= 0.94531\n",
      "Iter 44800, Minibatch Loss= 290.396790, Training Accuracy= 0.96875\n",
      "Iter 46080, Minibatch Loss= 419.617432, Training Accuracy= 0.93750\n",
      "Iter 47360, Minibatch Loss= 427.763550, Training Accuracy= 0.93750\n",
      "Iter 48640, Minibatch Loss= 637.451355, Training Accuracy= 0.94531\n",
      "Iter 49920, Minibatch Loss= 564.523804, Training Accuracy= 0.92969\n",
      "Iter 51200, Minibatch Loss= 557.136780, Training Accuracy= 0.94531\n",
      "Iter 52480, Minibatch Loss= 793.874329, Training Accuracy= 0.92188\n",
      "Iter 53760, Minibatch Loss= 519.429626, Training Accuracy= 0.94531\n",
      "Iter 55040, Minibatch Loss= 71.447754, Training Accuracy= 0.97656\n",
      "Iter 56320, Minibatch Loss= 625.613281, Training Accuracy= 0.93750\n",
      "Iter 57600, Minibatch Loss= 551.920044, Training Accuracy= 0.96094\n",
      "Iter 58880, Minibatch Loss= 616.853333, Training Accuracy= 0.92969\n",
      "Iter 60160, Minibatch Loss= 339.829620, Training Accuracy= 0.97656\n",
      "Iter 61440, Minibatch Loss= 526.408386, Training Accuracy= 0.95312\n",
      "Iter 62720, Minibatch Loss= 138.304779, Training Accuracy= 0.97656\n",
      "Iter 64000, Minibatch Loss= 503.944977, Training Accuracy= 0.95312\n",
      "Iter 65280, Minibatch Loss= 246.436600, Training Accuracy= 0.96094\n",
      "Iter 66560, Minibatch Loss= 169.247925, Training Accuracy= 0.97656\n",
      "Iter 67840, Minibatch Loss= 617.076416, Training Accuracy= 0.96094\n",
      "Iter 69120, Minibatch Loss= 186.639221, Training Accuracy= 0.97656\n",
      "Iter 70400, Minibatch Loss= 257.008484, Training Accuracy= 0.96875\n",
      "Iter 71680, Minibatch Loss= 572.165527, Training Accuracy= 0.96094\n",
      "Iter 72960, Minibatch Loss= 418.070129, Training Accuracy= 0.98438\n",
      "Iter 74240, Minibatch Loss= 15.123627, Training Accuracy= 0.98438\n",
      "Iter 75520, Minibatch Loss= 147.908325, Training Accuracy= 0.98438\n",
      "Iter 76800, Minibatch Loss= 227.506424, Training Accuracy= 0.97656\n",
      "Iter 78080, Minibatch Loss= 78.672043, Training Accuracy= 0.98438\n",
      "Iter 79360, Minibatch Loss= 123.315720, Training Accuracy= 0.99219\n",
      "Iter 80640, Minibatch Loss= 306.196869, Training Accuracy= 0.96094\n",
      "Iter 81920, Minibatch Loss= 608.887390, Training Accuracy= 0.93750\n",
      "Iter 83200, Minibatch Loss= 781.018860, Training Accuracy= 0.91406\n",
      "Iter 84480, Minibatch Loss= 308.639526, Training Accuracy= 0.96094\n",
      "Iter 85760, Minibatch Loss= 179.107056, Training Accuracy= 0.97656\n",
      "Iter 87040, Minibatch Loss= 104.384705, Training Accuracy= 0.96094\n",
      "Iter 88320, Minibatch Loss= 359.779938, Training Accuracy= 0.96094\n",
      "Iter 89600, Minibatch Loss= 797.637573, Training Accuracy= 0.91406\n",
      "Iter 90880, Minibatch Loss= 216.301361, Training Accuracy= 0.97656\n",
      "Iter 92160, Minibatch Loss= 153.083847, Training Accuracy= 0.96875\n",
      "Iter 93440, Minibatch Loss= 495.094086, Training Accuracy= 0.96875\n",
      "Iter 94720, Minibatch Loss= 478.313202, Training Accuracy= 0.95312\n",
      "Iter 96000, Minibatch Loss= 372.452057, Training Accuracy= 0.94531\n",
      "Iter 97280, Minibatch Loss= 227.487656, Training Accuracy= 0.96875\n",
      "Iter 98560, Minibatch Loss= 319.718201, Training Accuracy= 0.96094\n",
      "Iter 99840, Minibatch Loss= 405.095825, Training Accuracy= 0.93750\n",
      "Iter 101120, Minibatch Loss= 776.669189, Training Accuracy= 0.94531\n",
      "Iter 102400, Minibatch Loss= 334.001556, Training Accuracy= 0.95312\n",
      "Iter 103680, Minibatch Loss= 215.854843, Training Accuracy= 0.96094\n",
      "Iter 104960, Minibatch Loss= 258.527710, Training Accuracy= 0.96094\n",
      "Iter 106240, Minibatch Loss= 270.684784, Training Accuracy= 0.97656\n",
      "Iter 107520, Minibatch Loss= 458.773376, Training Accuracy= 0.96094\n",
      "Iter 108800, Minibatch Loss= 321.841736, Training Accuracy= 0.96094\n",
      "Iter 110080, Minibatch Loss= 218.383911, Training Accuracy= 0.96094\n",
      "Iter 111360, Minibatch Loss= 183.723938, Training Accuracy= 0.96875\n",
      "Iter 112640, Minibatch Loss= 58.286041, Training Accuracy= 0.97656\n",
      "Iter 113920, Minibatch Loss= 178.331711, Training Accuracy= 0.97656\n",
      "Iter 115200, Minibatch Loss= 80.119324, Training Accuracy= 0.98438\n",
      "Iter 116480, Minibatch Loss= 54.229568, Training Accuracy= 0.99219\n",
      "Iter 117760, Minibatch Loss= 212.971817, Training Accuracy= 0.96875\n",
      "Iter 119040, Minibatch Loss= 180.338364, Training Accuracy= 0.95312\n",
      "Iter 120320, Minibatch Loss= 172.649155, Training Accuracy= 0.97656\n",
      "Iter 121600, Minibatch Loss= 176.536469, Training Accuracy= 0.96875\n",
      "Iter 122880, Minibatch Loss= 100.836449, Training Accuracy= 0.96875\n",
      "Iter 124160, Minibatch Loss= 143.886612, Training Accuracy= 0.97656\n",
      "Iter 125440, Minibatch Loss= 308.206787, Training Accuracy= 0.96094\n",
      "Iter 126720, Minibatch Loss= 267.640991, Training Accuracy= 0.96094\n",
      "Iter 128000, Minibatch Loss= 360.943207, Training Accuracy= 0.96875\n",
      "Iter 129280, Minibatch Loss= 152.790344, Training Accuracy= 0.96875\n",
      "Iter 130560, Minibatch Loss= 306.747864, Training Accuracy= 0.97656\n",
      "Iter 131840, Minibatch Loss= 371.814972, Training Accuracy= 0.97656\n",
      "Iter 133120, Minibatch Loss= 60.988892, Training Accuracy= 0.98438\n",
      "Iter 134400, Minibatch Loss= 218.858444, Training Accuracy= 0.94531\n",
      "Iter 135680, Minibatch Loss= 495.377380, Training Accuracy= 0.96094\n",
      "Iter 136960, Minibatch Loss= 53.276093, Training Accuracy= 0.98438\n",
      "Iter 138240, Minibatch Loss= 208.652191, Training Accuracy= 0.96094\n",
      "Iter 139520, Minibatch Loss= 402.648529, Training Accuracy= 0.96094\n",
      "Iter 140800, Minibatch Loss= 379.947754, Training Accuracy= 0.94531\n",
      "Iter 142080, Minibatch Loss= 242.451157, Training Accuracy= 0.96875\n",
      "Iter 143360, Minibatch Loss= 95.784241, Training Accuracy= 0.98438\n",
      "Iter 144640, Minibatch Loss= 512.619324, Training Accuracy= 0.96094\n",
      "Iter 145920, Minibatch Loss= 171.341187, Training Accuracy= 0.98438\n",
      "Iter 147200, Minibatch Loss= 88.423866, Training Accuracy= 0.97656\n",
      "Iter 148480, Minibatch Loss= 423.718658, Training Accuracy= 0.94531\n",
      "Iter 149760, Minibatch Loss= 469.721008, Training Accuracy= 0.95312\n",
      "Iter 151040, Minibatch Loss= 93.906975, Training Accuracy= 0.97656\n",
      "Iter 152320, Minibatch Loss= 282.940674, Training Accuracy= 0.93750\n",
      "Iter 153600, Minibatch Loss= 333.915863, Training Accuracy= 0.95312\n",
      "Iter 154880, Minibatch Loss= 101.098625, Training Accuracy= 0.98438\n",
      "Iter 156160, Minibatch Loss= 84.967255, Training Accuracy= 0.98438\n",
      "Iter 157440, Minibatch Loss= 83.581055, Training Accuracy= 0.97656\n",
      "Iter 158720, Minibatch Loss= 750.157227, Training Accuracy= 0.94531\n",
      "Iter 160000, Minibatch Loss= 219.257507, Training Accuracy= 0.96875\n",
      "Iter 161280, Minibatch Loss= 80.616005, Training Accuracy= 0.97656\n",
      "Iter 162560, Minibatch Loss= 61.214661, Training Accuracy= 0.99219\n",
      "Iter 163840, Minibatch Loss= 10.340408, Training Accuracy= 0.99219\n",
      "Iter 165120, Minibatch Loss= 389.843170, Training Accuracy= 0.96875\n",
      "Iter 166400, Minibatch Loss= 25.990555, Training Accuracy= 0.98438\n",
      "Iter 167680, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 168960, Minibatch Loss= 43.083069, Training Accuracy= 0.98438\n",
      "Iter 170240, Minibatch Loss= 38.451935, Training Accuracy= 0.98438\n",
      "Iter 171520, Minibatch Loss= 117.493027, Training Accuracy= 0.97656\n",
      "Iter 172800, Minibatch Loss= 110.101280, Training Accuracy= 0.96875\n",
      "Iter 174080, Minibatch Loss= 248.274826, Training Accuracy= 0.97656\n",
      "Iter 175360, Minibatch Loss= 11.443787, Training Accuracy= 0.97656\n",
      "Iter 176640, Minibatch Loss= 347.756348, Training Accuracy= 0.98438\n",
      "Iter 177920, Minibatch Loss= 42.857986, Training Accuracy= 0.98438\n",
      "Iter 179200, Minibatch Loss= 197.819397, Training Accuracy= 0.97656\n",
      "Iter 180480, Minibatch Loss= 304.255493, Training Accuracy= 0.96094\n",
      "Iter 181760, Minibatch Loss= 307.764801, Training Accuracy= 0.94531\n",
      "Iter 183040, Minibatch Loss= 40.416199, Training Accuracy= 0.96875\n",
      "Iter 184320, Minibatch Loss= 83.812363, Training Accuracy= 0.99219\n",
      "Iter 185600, Minibatch Loss= 155.095978, Training Accuracy= 0.98438\n",
      "Iter 186880, Minibatch Loss= 142.064453, Training Accuracy= 0.97656\n",
      "Iter 188160, Minibatch Loss= 356.520386, Training Accuracy= 0.94531\n",
      "Iter 189440, Minibatch Loss= 464.792084, Training Accuracy= 0.94531\n",
      "Iter 190720, Minibatch Loss= 39.601593, Training Accuracy= 0.98438\n",
      "Iter 192000, Minibatch Loss= 318.048218, Training Accuracy= 0.95312\n",
      "Iter 193280, Minibatch Loss= 73.196999, Training Accuracy= 0.98438\n",
      "Iter 194560, Minibatch Loss= 173.188995, Training Accuracy= 0.97656\n",
      "Iter 195840, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 197120, Minibatch Loss= 337.889404, Training Accuracy= 0.96094\n",
      "Iter 198400, Minibatch Loss= 39.994873, Training Accuracy= 0.99219\n",
      "Iter 199680, Minibatch Loss= 15.017281, Training Accuracy= 0.96875\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.96484375\n",
      "Saved Model\n",
      "Read model and Save to Local\n",
      "Completed Write\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 123\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "3 items cleaning up...\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_f7c54ae2e9b21df2aa5e81d647adb3413487cf8ff87a3e9909807b4608c61e90_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "Entering job release. Current time:2020-07-13T16:09:12.881141\n",
      "Starting job release. Current time:2020-07-13T16:09:13.668810\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 547\n",
      "[2020-07-13T16:09:13.678456] Entering context manager injector.\n",
      "Job release is complete. Current time:2020-07-13T16:09:14.543072\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: tf-mnist_1594656213_4e742f4a\n",
      "Web View: https://ml.azure.com/experiments/tf-mnist/runs/tf-mnist_1594656213_4e742f4a?wsid=/subscriptions/c46a9435-c957-4e6c-a0f4-b9a597984773/resourcegroups/mlops/workspaces/gputraining\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'tf-mnist_1594656213_4e742f4a',\n",
       " 'target': 'gpucluster1',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-07-13T16:06:01.520929Z',\n",
       " 'endTimeUtc': '2020-07-13T16:09:27.97991Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': '73de3149-2923-4a04-bde2-41fe36c78f3b',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
       " 'inputDatasets': [{'dataset': {'id': '7b573201-f261-434b-bb36-a102a6dcfe45'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'mnist', 'mechanism': 'Mount'}}],\n",
       " 'runDefinition': {'script': 'tf_mnist.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--data-folder',\n",
       "   'DatasetConsumptionConfig:mnist',\n",
       "   '--batch-size',\n",
       "   '50',\n",
       "   '--first-layer-neurons',\n",
       "   '300',\n",
       "   '--second-layer-neurons',\n",
       "   '100',\n",
       "   '--learning-rate',\n",
       "   '0.01'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'gpucluster1',\n",
       "  'dataReferences': {},\n",
       "  'data': {'mnist': {'dataLocation': {'dataset': {'id': '7b573201-f261-434b-bb36-a102a6dcfe45',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Mount',\n",
       "    'environmentVariableName': 'mnist',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'outputData': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment tf-mnist Environment',\n",
       "   'version': 'Autosave_2020-07-11T23:27:48Z_8f2c3c65',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-dataprep[pandas,fuse]',\n",
       "        'azureml-defaults',\n",
       "        'tensorflow-gpu==1.13.1',\n",
       "        'horovod==0.16.1']}],\n",
       "     'name': 'azureml_c0e081827c5441d25ef4420172ac8ce1'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'itpCompute': {'configuration': {}},\n",
       "  'cmAksCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_f7c54ae2e9b21df2aa5e81d647adb3413487cf8ff87a3e9909807b4608c61e90_d.txt': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594656213_4e742f4a/azureml-logs/55_azureml-execution-tvmps_f7c54ae2e9b21df2aa5e81d647adb3413487cf8ff87a3e9909807b4608c61e90_d.txt?sv=2019-02-02&sr=b&sig=7PoJgzYj%2BAqEA9LFFAq3CmoVSvRgqnTyuUqSGv3M7oo%3D&st=2020-07-13T15%3A59%3A28Z&se=2020-07-14T00%3A09%3A28Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_f7c54ae2e9b21df2aa5e81d647adb3413487cf8ff87a3e9909807b4608c61e90_d.txt': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594656213_4e742f4a/azureml-logs/65_job_prep-tvmps_f7c54ae2e9b21df2aa5e81d647adb3413487cf8ff87a3e9909807b4608c61e90_d.txt?sv=2019-02-02&sr=b&sig=hVj464KQPd%2FDrw5Rm4UO%2FaeiK1tbquTP0uyYdPY5SNQ%3D&st=2020-07-13T15%3A59%3A28Z&se=2020-07-14T00%3A09%3A28Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594656213_4e742f4a/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=TURvYZ%2BcyN%2FoUCxsuMTJ%2F0nChCxnDxM7%2F3XH6mNKeHw%3D&st=2020-07-13T15%3A59%3A28Z&se=2020-07-14T00%3A09%3A28Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_f7c54ae2e9b21df2aa5e81d647adb3413487cf8ff87a3e9909807b4608c61e90_d.txt': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594656213_4e742f4a/azureml-logs/75_job_post-tvmps_f7c54ae2e9b21df2aa5e81d647adb3413487cf8ff87a3e9909807b4608c61e90_d.txt?sv=2019-02-02&sr=b&sig=2%2BoCLH9u12s2s3wixHoMWE%2Bupj8hbdUWPa4N0J9v2ZU%3D&st=2020-07-13T15%3A59%3A28Z&se=2020-07-14T00%3A09%3A28Z&sp=r',\n",
       "  'azureml-logs/process_info.json': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594656213_4e742f4a/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=I21mpVL47z25Ozi0AoyNUW%2FeXMIGyUvJPK95cgIejDA%3D&st=2020-07-13T15%3A59%3A28Z&se=2020-07-14T00%3A09%3A28Z&sp=r',\n",
       "  'azureml-logs/process_status.json': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594656213_4e742f4a/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=71C4CH8khnDFQWD3MOSsVH6dxslcDKXm6UiijV2qMOo%3D&st=2020-07-13T15%3A59%3A28Z&se=2020-07-14T00%3A09%3A28Z&sp=r',\n",
       "  'logs/azureml/123_azureml.log': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594656213_4e742f4a/logs/azureml/123_azureml.log?sv=2019-02-02&sr=b&sig=V%2BkTrH9YaJzwTLnxhMQn0Vb%2F%2FX%2BtKQa%2F%2F%2FOO4JdLbNk%3D&st=2020-07-13T15%3A59%3A28Z&se=2020-07-14T00%3A09%3A28Z&sp=r',\n",
       "  'logs/azureml/job_prep_azureml.log': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594656213_4e742f4a/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=p2bh9woX5aUhoLp9KyyXxPDzoyNiuEv%2B52iui93mgzs%3D&st=2020-07-13T15%3A59%3A28Z&se=2020-07-14T00%3A09%3A28Z&sp=r',\n",
       "  'logs/azureml/job_release_azureml.log': 'https://gputraining4139219777.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1594656213_4e742f4a/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=Vb2pSssuk2GMfCivAVaw7yqVJKsAZcxeqoJCH3c5hqo%3D&st=2020-07-13T15%3A59%3A28Z&se=2020-07-14T00%3A09%3A28Z&sp=r'}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(est)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azureml-logs/55_azureml-execution-tvmps_f7c54ae2e9b21df2aa5e81d647adb3413487cf8ff87a3e9909807b4608c61e90_d.txt', 'azureml-logs/65_job_prep-tvmps_f7c54ae2e9b21df2aa5e81d647adb3413487cf8ff87a3e9909807b4608c61e90_d.txt', 'azureml-logs/70_driver_log.txt', 'azureml-logs/75_job_post-tvmps_f7c54ae2e9b21df2aa5e81d647adb3413487cf8ff87a3e9909807b4608c61e90_d.txt', 'azureml-logs/process_info.json', 'azureml-logs/process_status.json', 'logs/azureml/123_azureml.log', 'logs/azureml/job_prep_azureml.log', 'logs/azureml/job_release_azureml.log', 'outputs/model/checkpoint', 'outputs/model/tf-dnn-mnist.data-00000-of-00001', 'outputs/model/tf-dnn-mnist.index', 'outputs/model/tf-dnn-mnist.meta']\n"
     ]
    }
   ],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs('./outputs', exist_ok=True)\n",
    "#os.makedirs('./outputs/model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "model = run.register_model(model_name='tf-dnn-mnist', \n",
    "                           model_path='outputs/model',\n",
    "                           model_framework=Model.Framework.TENSORFLOW,\n",
    "                           model_framework_version='1.13.0',\n",
    "                           resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model folder in the current directory\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    if f.startswith('outputs/model'):\n",
    "        output_file_path = os.path.join('./model', f.split('/')[-1])\n",
    "        print('Downloading from {} to {} ...'.format(f, output_file_path))\n",
    "        run.download_file(name=f, output_file_path=output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "# Tensorflow constructor\n",
    "estimator= TensorFlow(source_directory=project_folder,\n",
    "                      compute_target=compute_target,\n",
    "                      script_params=script_params,\n",
    "                      entry_script='script.py',\n",
    "                      node_count=2,\n",
    "                      process_count_per_node=1,\n",
    "                      distributed_training=MpiConfiguration(),\n",
    "                      framework_version='1.13',\n",
    "                      use_gpu=True,\n",
    "                      pip_packages=['azureml-dataprep[pandas,fuse]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "distributed_training = TensorflowConfiguration()\n",
    "distributed_training.worker_count = 2\n",
    "\n",
    "# Tensorflow constructor\n",
    "tf_est= TensorFlow(source_directory=project_folder,\n",
    "                      compute_target=compute_target,\n",
    "                      script_params=script_params,\n",
    "                      entry_script='script.py',\n",
    "                      node_count=2,\n",
    "                      process_count_per_node=1,\n",
    "                      distributed_training=distributed_training,\n",
    "                      use_gpu=True,\n",
    "                      pip_packages=['azureml-dataprep[pandas,fuse]'])\n",
    "\n",
    "# submit the TensorFlow job\n",
    "run = exp.submit(tf_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_CONFIG='{\n",
    "    \"cluster\": {\n",
    "        \"ps\": [\"host0:2222\", \"host1:2222\"],\n",
    "        \"worker\": [\"host2:2222\", \"host3:2222\", \"host4:2222\"],\n",
    "    },\n",
    "    \"task\": {\"type\": \"ps\", \"index\": 0},\n",
    "    \"environment\": \"cloud\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_config = os.environ.get('TF_CONFIG')\n",
    "if not tf_config or tf_config == \"\":\n",
    "    raise ValueError(\"TF_CONFIG not found.\")\n",
    "tf_config_json = json.loads(tf_config)\n",
    "cluster_spec = tf.train.ClusterSpec(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Model.deploy(ws, \"tensorflow-web-service\", [model])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
